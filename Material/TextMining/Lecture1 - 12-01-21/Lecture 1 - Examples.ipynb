{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 0: working with text in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is a comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" this is also a comment\n",
    "possibly over several lines\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(\"99\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(99)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic string operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate strings\n",
    "str1 = 'ooh'\n",
    "str2 = \"how nice\"\n",
    "str1 + str2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate strings\n",
    "str3 = str1 + \" \" + str2\n",
    "str3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate strings\n",
    "str1 * 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# length of string\n",
    "letters = \"abcdefghijk\"\n",
    "len(letters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first character \n",
    "letters[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first 4 characters + all characters starting from the 5th  \n",
    "letters[0:4] + letters[4:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all characters starting from the 5th  \n",
    "letters[4:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# last 3 characters\n",
    "letters[-3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from first character, to the 7th character, by every two characters\n",
    "letters[0:8:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# length of string\n",
    "sent = \"this is my first list of words; nothing special about them I say to my self \\n\"\n",
    "len(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split string\n",
    "lsent = sent.split()\n",
    "lsent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join strings\n",
    "\" \".join(lsent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# starts with?\n",
    "sent.startswith(\"this\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ends with?\n",
    "sent.endswith('him')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# contains?\n",
    "sent.find('my')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count occurrences\n",
    "sent.count('my')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# contains only alphanumeric characters?\n",
    "sent.isalpha()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removes leading and trailing whitespaces\n",
    "sent.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removes leading characters\n",
    "sent.strip('t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# capitalize first letter\n",
    "sent.capitalize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# capitalize first letter of each word\n",
    "sent.title()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to uppercase\n",
    "sent.upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to lowercase\n",
    "sent.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# swap lower with upper case\n",
    "sent.swapcase()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace occurrence\n",
    "sent.replace('my',\"mad\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Python structures: Lists, Tuples, Dictionaries, Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lists\n",
    "dlist = ['Monday','Tuesday','Wednesday','Thursday','Friday','Saturday','Sunday']\n",
    "mlist = ['January','February','March','April','May','June']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first element\n",
    "dlist[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of lists\n",
    "calend = [dlist,mlist]\n",
    "calend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first element\n",
    "calend[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# third element of the first element\n",
    "calend[0][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# last element of the second element\n",
    "calend[1][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# append element to list\n",
    "mlist.append('July')\n",
    "mlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# append element to list\n",
    "dlist.append(mlist)\n",
    "dlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# second to last\n",
    "dlist[-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove all elements from second to last on\n",
    "del dlist[-2:]\n",
    "dlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extend list with elements in another list\n",
    "dlist.extend(mlist)\n",
    "dlist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lists are passed by reference: side effects!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you probably DON'T want to do this\n",
    "copied = dlist\n",
    "copied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change element in the copied list\n",
    "copied[0] = 'surprise'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# also the element of the original list has been modified! \n",
    "# the list is not copied but both the variable 'copied' and 'dlist' are actually the same object!\n",
    "dlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is safe to do\n",
    "right = dlist.copy() \n",
    "right[0] = 'This will not change the original list'\n",
    "dlist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sorting lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort list in ascending order\n",
    "dlist.sort()\n",
    "dlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort list in descending order\n",
    "dlist.sort(reverse = True)\n",
    "dlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return a copy of the sorted list \n",
    "mlist2 = sorted(mlist) \n",
    "print(mlist)\n",
    "print(mlist2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign a = 2 and b = 3\n",
    "(a,b) = (2,3) \n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# swap a with b\n",
    "b,a = a,b\n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a tuple\n",
    "x = a,b\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# length of a tuple\n",
    "len(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dictionaries (hash tables, unique keys): unlike list, order is arbitrary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionary\n",
    "dico = {'Anna':22 ,'Maria':35 ,'Laura':60, 'Michael':56}\n",
    "dico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# element by key \n",
    "dico['Anna']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keys\n",
    "dico.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# values\n",
    "dico.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# items\n",
    "dico.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop over dictionary keys\n",
    "for name in list(dico.keys()):\n",
    "    print(name, dico[name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort keys and loop \n",
    "for name in sorted(list(dico.keys())):\n",
    "    print(\"%15s has age %d\" % (name, dico[name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update dictionary with another dictionary\n",
    "dico.update({'Huang':44})\n",
    "dico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove element by key\n",
    "del dico['Anna']\n",
    "dico"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sets: no order, only unique elements. Dictionaries are particular types of sets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define sets\n",
    "evenNbrs = {2,4,6,8,10}\n",
    "otherset = {1,2,3,4,5}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# length of set\n",
    "len(evenNbrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# element in set?\n",
    "2 in evenNbrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# intersection\n",
    "otherset.intersection(evenNbrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# union\n",
    "otherset.union(evenNbrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# difference\n",
    "otherset.difference(evenNbrs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python flows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if-else\n",
    "dolearn = False\n",
    "\n",
    "if dolearn:\n",
    "    print('Will get a good grade')\n",
    "elif dolearn=='lazy':\n",
    "    print('lazy bone')\n",
    "else:\n",
    "    print('will fail')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# while loop\n",
    "count = 1\n",
    "while count <= 3: \n",
    "    print(count)\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for loop over characters\n",
    "word = 'Empirical Asset'\n",
    "for letter in word:\n",
    "    print(letter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for loop over tuples\n",
    "\n",
    "days = ['Monday', 'Tuesday', 'Wednesday']\n",
    "fruits = ['banana', 'orange', 'peach']\n",
    "drinks = ['coffee', 'tea', 'beer']\n",
    "desserts = ['tiramisu', 'ice cream', 'pie', 'pudding']\n",
    "\n",
    "for day, fruit, drink in zip(days, fruits, drinks):\n",
    "    print(day, \": drink\", drink, \"- eat\", fruit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Matlab type i=1:10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for loop over range\n",
    "for i in range(1,3):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for loop over range\n",
    "for i in range(1,10,2):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Elegant way to work with lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define list with for loop\n",
    "number_list = [number for number in range(1,10)]\n",
    "number_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop over list\n",
    "for i in number_list:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define list with for loop and condition\n",
    "evenlist = [number for number in range(1,10) if number%2==0]\n",
    "evenlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define list applying a function in for loop with condition\n",
    "\n",
    "def capi(x):\n",
    "    return x.swapcase()\n",
    "\n",
    "listop = [capi(x) for x in ['THIS', 'IS', 'amazing', '!'] if x!='!']\n",
    "listop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 1: splitting strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MIDWAY upon the journey of our life \n",
      "I found myself within a forest dark, \n",
      "For the straightforward pathway had been lost.  \n",
      "Ah me! how hard a thing it is to say \n",
      "What was this forest savage, rough, and stern,  \n",
      "Which in the very thought renews the fear.  \n",
      "So bitter is it, death is little more; \n",
      "But of the good to treat, which there I found,  \n",
      "Speak will I of the other things I saw there.  \n",
      "I cannot well repeat how there I entered,  \n",
      "So full was I of slumber at the moment  \n",
      "In which I had abandoned the true way.  \n"
     ]
    }
   ],
   "source": [
    "# Dante Alighieri's Divine Comedy in English\n",
    "# Loading, opening and reading text (or \"corpus\") file:\n",
    "filename = 'DivineComedy.txt'\n",
    "file = open(filename, 'rt')\n",
    "text = file.read()\n",
    "file.close()\n",
    "#Printing:\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['MIDWAY', 'upon', 'the', 'journey', 'of', 'our', 'life', 'I', 'found', 'myself', 'within', 'a', 'forest', 'dark,', 'For', 'the', 'straightforward', 'pathway', 'had', 'been', 'lost.', 'Ah', 'me!', 'how', 'hard', 'a', 'thing', 'it', 'is', 'to', 'say', 'What', 'was', 'this', 'forest', 'savage,', 'rough,', 'and', 'stern,', 'Which', 'in', 'the', 'very', 'thought', 'renews', 'the', 'fear.', 'So', 'bitter', 'is', 'it,', 'death', 'is', 'little', 'more;', 'But', 'of', 'the', 'good', 'to', 'treat,', 'which', 'there', 'I', 'found,', 'Speak', 'will', 'I', 'of', 'the', 'other', 'things', 'I', 'saw', 'there.', 'I', 'cannot', 'well', 'repeat', 'how', 'there', 'I', 'entered,', 'So', 'full', 'was', 'I', 'of', 'slumber', 'at', 'the', 'moment', 'In', 'which', 'I', 'had', 'abandoned', 'the', 'true', 'way.']\n"
     ]
    }
   ],
   "source": [
    "# splitting into words by white space\n",
    "words = text.split()\n",
    "print(words[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frequencies\n",
      "[1, 1, 8, 1, 4, 1, 1, 8, 1, 1, 1, 2, 2, 1, 1, 8, 1, 1, 2, 1, 1, 1, 1, 2, 1, 2, 1, 1, 3, 2, 1, 1, 2, 1, 2, 1, 1, 1, 1, 1, 1, 8, 1, 1, 1, 8, 1, 2, 1, 3, 1, 1, 3, 1, 1, 1, 4, 8, 1, 2, 1, 2, 2, 8, 1, 1, 1, 8, 4, 8, 1, 1, 8, 1, 1, 8, 1, 1, 1, 2, 2, 8, 1, 2, 1, 2, 8, 4, 1, 1, 8, 1, 1, 2, 8, 2, 1, 8, 1, 1]\n",
      "\n",
      "Pairs\n",
      "[('MIDWAY', 1), ('upon', 1), ('the', 8), ('journey', 1), ('of', 4), ('our', 1), ('life', 1), ('I', 8), ('found', 1), ('myself', 1), ('within', 1), ('a', 2), ('forest', 2), ('dark,', 1), ('For', 1), ('the', 8), ('straightforward', 1), ('pathway', 1), ('had', 2), ('been', 1), ('lost.', 1), ('Ah', 1), ('me!', 1), ('how', 2), ('hard', 1), ('a', 2), ('thing', 1), ('it', 1), ('is', 3), ('to', 2), ('say', 1), ('What', 1), ('was', 2), ('this', 1), ('forest', 2), ('savage,', 1), ('rough,', 1), ('and', 1), ('stern,', 1), ('Which', 1), ('in', 1), ('the', 8), ('very', 1), ('thought', 1), ('renews', 1), ('the', 8), ('fear.', 1), ('So', 2), ('bitter', 1), ('is', 3), ('it,', 1), ('death', 1), ('is', 3), ('little', 1), ('more;', 1), ('But', 1), ('of', 4), ('the', 8), ('good', 1), ('to', 2), ('treat,', 1), ('which', 2), ('there', 2), ('I', 8), ('found,', 1), ('Speak', 1), ('will', 1), ('I', 8), ('of', 4), ('the', 8), ('other', 1), ('things', 1), ('I', 8), ('saw', 1), ('there.', 1), ('I', 8), ('cannot', 1), ('well', 1), ('repeat', 1), ('how', 2), ('there', 2), ('I', 8), ('entered,', 1), ('So', 2), ('full', 1), ('was', 2), ('I', 8), ('of', 4), ('slumber', 1), ('at', 1), ('the', 8), ('moment', 1), ('In', 1), ('which', 2), ('I', 8), ('had', 2), ('abandoned', 1), ('the', 8), ('true', 1), ('way.', 1)]\n"
     ]
    }
   ],
   "source": [
    "# Counting words:\n",
    "wordfreq = []\n",
    "for w in words:\n",
    "    wordfreq.append(words.count(w))\n",
    "# Displaying words' frequencies:\n",
    "print(\"Frequencies\\n\" + str(wordfreq) + \"\\n\")\n",
    "print(\"Pairs\\n\" + str(list(zip(words, wordfreq))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 2: conversion from hex to dec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From an example in wikipedia: conversion from integer (<16) to hex\n",
    "def toHex(d):\n",
    "    if (d < 16):\n",
    "        res = d % 16\n",
    "        if (d - res == 0):\n",
    "            return toChar(res)\n",
    "        else:\n",
    "            return (((d - res) / 16) + toChar(res))\n",
    "    else:\n",
    "        print('please, type an integer 0-15')\n",
    "\n",
    "def toChar(n):\n",
    "    const_alpha = \"0123456789ABCDEF\"\n",
    "    return const_alpha[n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "please, type an integer 0-15\n"
     ]
    }
   ],
   "source": [
    "toHex(117)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 3: Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0x80 in position 19: invalid start byte",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-dec8d435c603>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'crncy-ansi.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/anaconda3/envs/EnvTextMining/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    603\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 605\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/EnvTextMining/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/EnvTextMining/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    812\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 814\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    815\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    816\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/EnvTextMining/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1043\u001b[0m             )\n\u001b[1;32m   1044\u001b[0m         \u001b[0;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1045\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1046\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1047\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/EnvTextMining/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1891\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1892\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1893\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1894\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1895\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._get_header\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0x80 in position 19: invalid start byte"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.read_csv('crncy-ansi.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>crncy</th>\n",
       "      <th>symbol</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>euro</td>\n",
       "      <td>Ç</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dollar</td>\n",
       "      <td>$</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>pound</td>\n",
       "      <td>ú</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    crncy symbol\n",
       "0    euro      Ç\n",
       "1  dollar      $\n",
       "2   pound      ú"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read in the correct encoding\n",
    "crncy = pd.read_csv('crncy-ansi.csv', sep =',', encoding='cp850')\n",
    "crncy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write file in UTF-8 (default in Python 3)\n",
    "crncy.to_csv('crncy.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>crncy</th>\n",
       "      <th>symbol</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>euro</td>\n",
       "      <td>Ç</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dollar</td>\n",
       "      <td>$</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>pound</td>\n",
       "      <td>ú</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    crncy symbol\n",
       "0    euro      Ç\n",
       "1  dollar      $\n",
       "2   pound      ú"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read from UTF-8 (default in Python 3)\n",
    "pd.read_csv('crncy.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "review    Quando ho avuto problemi con la restituzione della bici l'assistenza è stata molto gentile, ma non ha potuto risolvere velocemente il problema nè evitare che mi venisse addebitato il costo del noleggio perchè avevo superato la mezz'ora (non a causa del percorso da me effettuato, ma a causa dell'impossibilità di riagganciare la bici in 2 stazioni diverse). Sarebbe auspicabile poter risolvere i problemi più efficientemente o almeno non far pagare l'utente in caso di segnalato disservizio. Per il resto, quando il servizio funziona, è davvero pratico e utile.\n",
      "Name: 0, dtype: object\n"
     ]
    }
   ],
   "source": [
    "pd.set_option('display.max_colwidth', None)\n",
    "BikeMi = pd.read_csv('opinion.csv', sep=';')\n",
    "BikeMi = pd.DataFrame(BikeMi)\n",
    "print(BikeMi.iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 4: Regular Expressions (finding characters in a text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitt_text = \"Canadian Prime Minister Justin Trudeau on the Trump loyalist #CapitolBuilding attack: 'What we witnessed was an assault on democracy by violent rioters, incited by the current president and other politicians.#Capitol\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['#CapitolBuilding', '#Capitol']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "re.findall(r'#\\w+', twitt_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Trudeau', 'Trump']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Now try to find all words beginning with 'T'\n",
    "re.findall(r'T\\w+', twitt_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['witnessed', 'incited']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Now find all the regular past clauses\n",
    "re.findall(r'\\w+ed', twitt_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 5: Regular Expressions (finding characters in a text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitt_text2 = \"Citizens protest @BorisJohnson’s latest thoughtless lockdown in #Clapham, #London. #LockdownLevel3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['@BorisJohnson']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Now try to find all words beginning with '@'\n",
    "re.findall(r'@\\w+', twitt_text2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['@BorisJohnson', '#Clapham', '#London', '#LockdownLevel3']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Now try to find callouts and hashtags\n",
    "re.findall(r'(@\\w+|#\\w+)', twitt_text2)\n",
    "#'(iPhone|iPad)'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 6: Regular Expressions (Matching characters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Capitol']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "twitt_text3 = \"Instead, we see local FOP leaders like John Catanzara in Chicago sympathizing with the white supremacists who stormed the Capitol. The same man who suggested cops could get away with shooting Black people engaged in peaceful protests last summer.\"\n",
    "\n",
    "#Searching for a string starting with \"Cap\" and ending with \"ol\":\n",
    "x = re.findall(\"Cap..ol\", twitt_text3)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes, the string ends with 'summer.'\n"
     ]
    }
   ],
   "source": [
    "#Searching for a string ending with \"ed\":\n",
    "\n",
    "x = re.findall(\"summer.$\", twitt_text3)\n",
    "if x:\n",
    "  print(\"Yes, the string ends with 'summer.'\")\n",
    "else:\n",
    "  print(\"No match\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 7: Regular Expressions (Character symbols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'search' is used for searching characters within strings. Output is the position of the searched character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first white-space character is located in position: 9\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "# Search for the first white-space character in a twitter message:\n",
    "twitt_text4 = \"According to the tick-tock of Wednesday’s siege, Capitol Police requested National Guard support at 2:22 p.m. But support from the Guard didn’t arrive until 5:45 p.m.What explains the 3 hour delay?\"\n",
    "x = re.search(\"\\s\", twitt_text4)\n",
    "print(\"The first white-space character is located in position:\", x.start())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 8: Regular Expressions (Quantifiers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes, we have two 'l's\n"
     ]
    }
   ],
   "source": [
    "BritishEn = \"modelling\" #British English\n",
    "AmericanEn = \"modeling\" #American English\n",
    "x = re.search(\"l{2}\", BritishEn) # Example 9: Basic Regular Expressions (Other examples) En)\n",
    "if x:\n",
    "    print(\"Yes, we have two 'l's\")\n",
    "else:\n",
    "    print(\"No, we don't have two 'l's\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 9: Basic Regular Expressions (Other examples) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ay']\n"
     ]
    }
   ],
   "source": [
    "# Print a list of all matches:\n",
    "str = \"The rain in Spain stays mainly in the plain\"\n",
    "x = re.findall(\"ay\", str)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pronounciation of \"h\" in British English (from \"My fair lady\" movie - https://www.youtube.com/watch?v=HOhVdoxrTvA, start from sec 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['H', 'H', 'H', 'h', 'h', 'h', 'h']\n"
     ]
    }
   ],
   "source": [
    "str = \"In Hertford, Hereford, Hampshire, hurricanes hardly ever happen\"\n",
    "x = re.findall(\"h|H\", str)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The split() Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The split() function returns a list where the string has been split at each match:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str = \"The rain in Spain stays mainly in the plain\"\n",
    "x = re.split(\"\\s\", str)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can control the number of occurrences by specifying the maxsplit parameter:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The sub() Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sub() function replaces the matches with the text of your choice:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In_Hertford,_Hereford,_Hampshire,_hurricanes_hardly_ever_happen\n"
     ]
    }
   ],
   "source": [
    "# Replace every white-space character with character '_':\n",
    "str = \"In Hertford, Hereford, Hampshire, hurricanes hardly ever happen\"\n",
    "x = re.sub(\"\\s\", \"_\", str)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can control the number of replacements by specifying the count parameter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In_Hertford,_Hereford, Hampshire, hurricanes hardly ever happen\n"
     ]
    }
   ],
   "source": [
    "# Replace the first 2 occurrences:\n",
    "str = \"In Hertford, Hereford, Hampshire, hurricanes hardly ever happen\"\n",
    "x = re.sub(\"\\s\", \"_\", str, 2)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Match Object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Match Object is an object containing information about the search and the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(3, 5), match='He'>\n"
     ]
    }
   ],
   "source": [
    "# Do a search that will return a Match Object:\n",
    "str = \"In Hertford, Hereford, Hampshire, hurricanes hardly ever happen\"\n",
    "x = re.search(\"He\", str)\n",
    "\n",
    "print(x)            # the object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 5)\n"
     ]
    }
   ],
   "source": [
    "print(x.span())     # a tuple containing the start-, and end positions of the match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In Hertford, Hereford, Hampshire, hurricanes hardly ever happen\n"
     ]
    }
   ],
   "source": [
    "print(x.string)     # the string passed into the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "He\n"
     ]
    }
   ],
   "source": [
    "print(x.group())    # the part of the string where there was a match"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 10: Tokenization with simple commands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Elvis',\n",
       " 'created',\n",
       " 'a',\n",
       " 'classic',\n",
       " 'rock',\n",
       " \"'n'\",\n",
       " 'roll',\n",
       " 'band.',\n",
       " 'I',\n",
       " \"don't\",\n",
       " 'think',\n",
       " 'there',\n",
       " 'is',\n",
       " 'a',\n",
       " 'musician',\n",
       " 'today',\n",
       " 'that',\n",
       " \"hasn't\",\n",
       " 'been',\n",
       " 'affected',\n",
       " 'by',\n",
       " \"Elvis'\",\n",
       " 'music']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt = \"Elvis created a classic rock 'n' roll band. I don't think there is a musician today that hasn't been affected by Elvis' music\"\n",
    "txt.split(' ') # Naive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/giancarlomanzi/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Elvis',\n",
       " 'created',\n",
       " 'a',\n",
       " 'classic',\n",
       " 'rock',\n",
       " \"'\",\n",
       " 'n',\n",
       " \"'\",\n",
       " 'roll',\n",
       " 'band',\n",
       " '.',\n",
       " 'I',\n",
       " 'do',\n",
       " \"n't\",\n",
       " 'think',\n",
       " 'there',\n",
       " 'is',\n",
       " 'a',\n",
       " 'musician',\n",
       " 'today',\n",
       " 'that',\n",
       " 'has',\n",
       " \"n't\",\n",
       " 'been',\n",
       " 'affected',\n",
       " 'by',\n",
       " 'Elvis',\n",
       " \"'\",\n",
       " 'music']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import nltk # NLTK has an in-built tokenizer! \n",
    "nltk.download('punkt')\n",
    "nltk.word_tokenize(txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 11: splitting long text in sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['MIDWAY upon the journey of our life, I found myself within a forest dark, for the straightforward pathway had been lost.',\n",
       " 'Ah me!',\n",
       " 'how hard a thing it is to say what was this forest savage, rough, and stern, which in the very thought renews the fear.',\n",
       " 'So bitter is it, death is little more; but of the good to treat, which there I found, speak will I of the other things I saw there.',\n",
       " 'I cannot well repeat how there I entered,  so full was I of slumber at the moment in which I had abandoned the true way']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "text = \"MIDWAY upon the journey of our life, I found myself within a forest dark, for the straightforward pathway had been lost.  Ah me! how hard a thing it is to say what was this forest savage, rough, and stern, which in the very thought renews the fear.  So bitter is it, death is little more; but of the good to treat, which there I found, speak will I of the other things I saw there. I cannot well repeat how there I entered,  so full was I of slumber at the moment in which I had abandoned the true way\"\n",
    "nltk.sent_tokenize(text)# Example 12: n-grams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 12: n-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('MIDWAY', 'upon', 'the', 'journey')\n",
      "('upon', 'the', 'journey', 'of')\n",
      "('the', 'journey', 'of', 'our')\n",
      "('journey', 'of', 'our', 'life')\n",
      "('of', 'our', 'life', ',')\n",
      "('our', 'life', ',', 'I')\n",
      "('life', ',', 'I', 'found')\n",
      "(',', 'I', 'found', 'myself')\n",
      "('I', 'found', 'myself', 'within')\n",
      "('found', 'myself', 'within', 'a')\n",
      "('myself', 'within', 'a', 'forest')\n",
      "('within', 'a', 'forest', 'dark')\n",
      "('a', 'forest', 'dark', ',')\n",
      "('forest', 'dark', ',', 'for')\n",
      "('dark', ',', 'for', 'the')\n",
      "(',', 'for', 'the', 'straightforward')\n",
      "('for', 'the', 'straightforward', 'pathway')\n",
      "('the', 'straightforward', 'pathway', 'had')\n",
      "('straightforward', 'pathway', 'had', 'been')\n",
      "('pathway', 'had', 'been', 'lost')\n",
      "('had', 'been', 'lost', '.')\n",
      "('been', 'lost', '.', 'Ah')\n",
      "('lost', '.', 'Ah', 'me')\n",
      "('.', 'Ah', 'me', '!')\n",
      "('Ah', 'me', '!', 'how')\n",
      "('me', '!', 'how', 'hard')\n",
      "('!', 'how', 'hard', 'a')\n",
      "('how', 'hard', 'a', 'thing')\n",
      "('hard', 'a', 'thing', 'it')\n",
      "('a', 'thing', 'it', 'is')\n",
      "('thing', 'it', 'is', 'to')\n",
      "('it', 'is', 'to', 'say')\n",
      "('is', 'to', 'say', 'what')\n",
      "('to', 'say', 'what', 'was')\n",
      "('say', 'what', 'was', 'this')\n",
      "('what', 'was', 'this', 'forest')\n",
      "('was', 'this', 'forest', 'savage')\n",
      "('this', 'forest', 'savage', ',')\n",
      "('forest', 'savage', ',', 'rough')\n",
      "('savage', ',', 'rough', ',')\n",
      "(',', 'rough', ',', 'and')\n",
      "('rough', ',', 'and', 'stern')\n",
      "(',', 'and', 'stern', ',')\n",
      "('and', 'stern', ',', 'which')\n",
      "('stern', ',', 'which', 'in')\n",
      "(',', 'which', 'in', 'the')\n",
      "('which', 'in', 'the', 'very')\n",
      "('in', 'the', 'very', 'thought')\n",
      "('the', 'very', 'thought', 'renews')\n",
      "('very', 'thought', 'renews', 'the')\n",
      "('thought', 'renews', 'the', 'fear')\n",
      "('renews', 'the', 'fear', '.')\n",
      "('the', 'fear', '.', 'So')\n",
      "('fear', '.', 'So', 'bitter')\n",
      "('.', 'So', 'bitter', 'is')\n",
      "('So', 'bitter', 'is', 'it')\n",
      "('bitter', 'is', 'it', ',')\n",
      "('is', 'it', ',', 'death')\n",
      "('it', ',', 'death', 'is')\n",
      "(',', 'death', 'is', 'little')\n",
      "('death', 'is', 'little', 'more')\n",
      "('is', 'little', 'more', ';')\n",
      "('little', 'more', ';', 'but')\n",
      "('more', ';', 'but', 'of')\n",
      "(';', 'but', 'of', 'the')\n",
      "('but', 'of', 'the', 'good')\n",
      "('of', 'the', 'good', 'to')\n",
      "('the', 'good', 'to', 'treat')\n",
      "('good', 'to', 'treat', ',')\n",
      "('to', 'treat', ',', 'which')\n",
      "('treat', ',', 'which', 'there')\n",
      "(',', 'which', 'there', 'I')\n",
      "('which', 'there', 'I', 'found')\n",
      "('there', 'I', 'found', ',')\n",
      "('I', 'found', ',', 'speak')\n",
      "('found', ',', 'speak', 'will')\n",
      "(',', 'speak', 'will', 'I')\n",
      "('speak', 'will', 'I', 'of')\n",
      "('will', 'I', 'of', 'the')\n",
      "('I', 'of', 'the', 'other')\n",
      "('of', 'the', 'other', 'things')\n",
      "('the', 'other', 'things', 'I')\n",
      "('other', 'things', 'I', 'saw')\n",
      "('things', 'I', 'saw', 'there')\n",
      "('I', 'saw', 'there', '.')\n",
      "('saw', 'there', '.', 'I')\n",
      "('there', '.', 'I', 'can')\n",
      "('.', 'I', 'can', 'not')\n",
      "('I', 'can', 'not', 'well')\n",
      "('can', 'not', 'well', 'repeat')\n",
      "('not', 'well', 'repeat', 'how')\n",
      "('well', 'repeat', 'how', 'there')\n",
      "('repeat', 'how', 'there', 'I')\n",
      "('how', 'there', 'I', 'entered')\n",
      "('there', 'I', 'entered', ',')\n",
      "('I', 'entered', ',', 'so')\n",
      "('entered', ',', 'so', 'full')\n",
      "(',', 'so', 'full', 'was')\n",
      "('so', 'full', 'was', 'I')\n",
      "('full', 'was', 'I', 'of')\n",
      "('was', 'I', 'of', 'slumber')\n",
      "('I', 'of', 'slumber', 'at')\n",
      "('of', 'slumber', 'at', 'the')\n",
      "('slumber', 'at', 'the', 'moment')\n",
      "('at', 'the', 'moment', 'in')\n",
      "('the', 'moment', 'in', 'which')\n",
      "('moment', 'in', 'which', 'I')\n",
      "('in', 'which', 'I', 'had')\n",
      "('which', 'I', 'had', 'abandoned')\n",
      "('I', 'had', 'abandoned', 'the')\n",
      "('had', 'abandoned', 'the', 'true')\n",
      "('abandoned', 'the', 'true', 'way')\n"
     ]
    }
   ],
   "source": [
    "bigrams = nltk.ngrams(nltk.word_tokenize(text), n = 4)\n",
    "for grams in bigrams: \n",
    "    print (grams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Su',\n",
       " 'per',\n",
       " 'ca',\n",
       " 'li',\n",
       " 'fra',\n",
       " 'gi',\n",
       " 'lis',\n",
       " 'ti',\n",
       " 'cex',\n",
       " 'pia',\n",
       " 'li',\n",
       " 'do',\n",
       " 'cio',\n",
       " 'us']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import SyllableTokenizer\n",
    "from nltk import word_tokenize \n",
    "text = \"Supercalifragilisticexpialidocious\"\n",
    "# Create a reference variable for Class word_tokenize \n",
    "tk = SyllableTokenizer() \n",
    "tk.tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'be': 2, 'To': 1, 'or': 1, 'not': 1, 'to': 1})"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"To be or not to be\"\n",
    "nltk.FreqDist(nltk.word_tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'to': 2, 'be': 2, 'or': 1, 'not': 1})"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.FreqDist(nltk.word_tokenize(text.lower()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 13: stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\",\n",
       " '!',\n",
       " '\"',\n",
       " '#',\n",
       " '$',\n",
       " '%',\n",
       " '&',\n",
       " \"'\",\n",
       " '(',\n",
       " ')',\n",
       " '*',\n",
       " '+',\n",
       " ',',\n",
       " '-',\n",
       " '.',\n",
       " '/',\n",
       " ':',\n",
       " ';',\n",
       " '<',\n",
       " '=',\n",
       " '>',\n",
       " '?',\n",
       " '@',\n",
       " '[',\n",
       " '\\\\',\n",
       " ']',\n",
       " '^',\n",
       " '_',\n",
       " '`',\n",
       " '{',\n",
       " '|',\n",
       " '}',\n",
       " '~']"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords \n",
    "import string\n",
    "stop_words_en = stopwords.words(\"english\") \n",
    "stop_words_it = stopwords.words(\"italian\")\n",
    "\n",
    "stop_words_en = stop_words_en + list(string.punctuation)\n",
    "stop_words_en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['house',\n",
       " 'first',\n",
       " 'try',\n",
       " 'force',\n",
       " 'mike',\n",
       " 'pence',\n",
       " 'oust',\n",
       " 'president',\n",
       " 'invoking',\n",
       " '25th',\n",
       " 'amendment',\n",
       " 'move',\n",
       " 'forward',\n",
       " 'impeachment']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk \n",
    "#Text from 'The Guardian', 11/1/2021\n",
    "text = \"House will first try to force Mike Pence to oust president by invoking 25th amendment and then move forward with impeachment\"\n",
    "\n",
    "[w for w in nltk.word_tokenize(text.lower()) \n",
    "     if w not in stop_words_en]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "EnvTextMining",
   "language": "python",
   "name": "envtextmining"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
