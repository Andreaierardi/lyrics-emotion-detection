{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Long Example 0: Text classification with pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### How samples are split in Scikit-learn $K$-fold CV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import scikit-learn libraries\n",
    "from sklearn.model_selection import KFold, StratifiedKFold, train_test_split, cross_validate, cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# import other libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the range 1 to 14\n",
    "rn = [1,2,3,4,5,6,7,8,9,10,11,12,13,14]\n",
    "print(rn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to demonstrate how the data are split, we will create 3 and 5 folds. \n",
    "# KFold function has to be applied on the data and it returns an \n",
    "# location (index) of the train and test samples.\n",
    "kf5 = KFold(n_splits=5, shuffle=False)\n",
    "kf3 = KFold(n_splits=3, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Kfold function retunrs the indices of the data. \n",
    "# Our range goes from 1-14 so the index is 0-13\n",
    "for train_index, test_index in kf3.split(rn):\n",
    "    print(train_index, test_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comment: we have here that n/k is 14/3, which is 4 with a reminder r=2.\n",
    "# The two reminder elements are assigned to the first two test folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K=5 folds:\n",
    "for train_index, test_index in kf5.split(rn):\n",
    "    print(train_index, test_index)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#Therefore the splitting is done by taking the value of the reminder r and \n",
    "#assigning one further element to each fold until r-th fold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipelines in scikit-learn (1)\n",
    "\n",
    "- We spent a lot of time in pre-processing and cleaning data\n",
    "- Therefore we need to create multipurpose software objects to be used in diffirent situations.\n",
    "- For this we can use the Pipeline tool in scikit-learn.\n",
    "- It is composed by *transformers* (tools for transforming data, for example to normalize a variable) and *estimators* (for example a fitting or predicting tool).\n",
    "- All transformers and estimators in scikit-learn are implemented as Python *classes*, each with their own attributes and methods.\n",
    "- We use *inherited* classes from scikit-learn to implement our own class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Pipeline.png\" width=\"500\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of inheritance\n",
    "from sklearn.preprocessing import OneHotEncoder \n",
    "#Some data:\n",
    "X = [[0, 0, 3], [1, 1, 0], [0, 2, 1], [1, 0, 2]]\n",
    "#Initializing an object of class OneHotEncoder\n",
    "# Here we are \"inheriting\" classes from OneHotEncoder into the variable 'one_hot_enc'\n",
    "one_hot_enc = OneHotEncoder( sparse = True )\n",
    "\n",
    "#Calling methods on our OneHotEncoder object\n",
    "one_hot_enc.fit( X ) #returns nothing\n",
    "transformed_data = one_hot_enc.transform(X).toarray() #returns something\n",
    "#fit_transformed_data = one_hot_enc.transform( X ) #returns something"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pd.DataFrame(X))\n",
    "#Comments: The first column takes on 2 values, the second 3 and the fourth 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(transformed_data)\n",
    "#Comments: the first two columns express the binary coding of the first \"feature\"; \n",
    "# the next three columns express the binary coding of the second \"feature\";\n",
    "# The next four columns express the binary coding of the third \"feature\";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipelines in scikit-learn (2)\n",
    "\n",
    "- Our own trasformer will be formed by inheriting from some other scikit-learn class.\n",
    "- See a tutorial here https://www.programiz.com/python-programming/class about classes and objects in python and a tutorial here https://www.programiz.com/python-programming/inheritance about inheritance.\n",
    "- The base classes inherited from scikit-learn are TransformerMixin (https://scikit-learn.org/stable/modules/generated/sklearn.base.TransformerMixin.html) and BaseEstimator (https://scikit-learn.org/stable/modules/generated/sklearn.base.BaseEstimator.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "BikeSent = pd.read_csv(\"BikeMiSentiment2019_UTF-8.csv\", sep=';')\n",
    "BikeSent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename columns\n",
    "BikeSent.columns = [\"target\", \"text\"]\n",
    "BikeSent.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode categories\n",
    "BikeSent['target'] = np.where(BikeSent['target']=='positive',1,0)\n",
    "BikeSent.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the sample in train (used also for cross-validation) + test \n",
    "from sklearn.model_selection import train_test_split\n",
    "X = BikeSent[['text']]\n",
    "y = BikeSent['target']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.10, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Transformers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Cleaning Text\n",
    "- We create here our own transformer (which will be a class) inheriting the TransformerMixin and the BaseEstimator classes from scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.base import TransformerMixin\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "# Custom Transformer (Inheriting from classes)\n",
    "class CleanText( BaseEstimator, TransformerMixin ):\n",
    "    \n",
    "    # Class Constructor \n",
    "    # The class constructor is formed by a function with double underscore __ :\n",
    "    # these are called 'special functions' as they have special meaning.\n",
    "    # In particular the '__init__' gets called whenever \n",
    "    # a new object of that class is instantiated,\n",
    "    # and are used to initialize all the necessary variables.\n",
    "    # In this example we initialize the language variable 'lang' with 'English'\n",
    "    # and pick the SnowballStemmer as the default stemmer.\n",
    "    def __init__( self, lang = \"english\"):\n",
    "        self.lang = lang\n",
    "        self.stemmer = SnowballStemmer(self.lang)\n",
    "    \n",
    "    # The 'fit' method here is used to instantiate the class on the 'self' variable \n",
    "    # and return the object itself     \n",
    "    def fit( self, X, y = None ):\n",
    "        return self \n",
    "    \n",
    "    # Custom function: this applies the stemmer just created in the '__init__'\n",
    "    # part to the 'self' variable\n",
    "    def clean( self, x ):\n",
    "        words   = [self.stemmer.stem(word) for word in word_tokenize(x.lower()) if word.isalpha() and word not in stopwords.words(\"english\")]\n",
    "        return \" \".join(words)\n",
    "    \n",
    "    # Method that describes what we need this transformer to do i.e. cleaning the text\n",
    "    # in the 'text' column in the data frame.\n",
    "    # This will be used later on in the usage of the custom transformer \n",
    "    # within the pipeline.\n",
    "    def transform( self, X, y = None ):\n",
    "        return X[\"text\"].apply(self.clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Transformer: same parts as the previous custom transformer\n",
    "# This one will be used for feature extraction\n",
    "\n",
    "class CustomFeatures( BaseEstimator, TransformerMixin ):\n",
    "    \n",
    "    # Class Constructor \n",
    "    def __init__( self ):\n",
    "        return\n",
    "    \n",
    "    # Return self nothing else to do here    \n",
    "    def fit( self, X, y = None ):\n",
    "        return self \n",
    "        \n",
    "    # Method that describes what we need this transformer to do i.e.\n",
    "    # returning length, digits and punctuations in the 'text' column in data frame\n",
    "    def transform( self, X, y = None ):\n",
    "        f           = pd.DataFrame()\n",
    "        f['len']    = X['text'].str.len()\n",
    "        f['digits'] = X['text'].str.findall(r'\\d').str.len()\n",
    "        f['punct']  = X['text'].str.findall(r'[^a-zA-Z\\d\\s:]').str.len()\n",
    "        return f[['len','digits','punct']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline usage\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline for data pre processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "# FeatureUnion combines two or more pipelines or transformers\n",
    "# and is very fast!\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# Our first pipeline called 'pipe' will be formed by three 'steps' or parts:\n",
    "# 1)\"extract\" which in turns is formed through FeatureUnion which put together two parts:\n",
    "# \"terms\" (formed by a pipeline with the CleanText() transformer we created above\n",
    "# and the TfidVectorize text vectorizing transformer from scikit-learn) and \"custom\" \n",
    "# (formed by the CustomFeatures transformer we created above);\n",
    "# 2) \"select\", formed by the scikit-learn transformer method \"SelectKBest\" for feature \n",
    "# selection with a chi squared score function;\n",
    "# 3) \"scale\", same as 2) using the StandardScaler method from scikit-learn. \n",
    "# The whole pipeline will be used as pre-processing task in classifying pipelines.\n",
    "pipe = Pipeline([(\"extract\", FeatureUnion([(\"terms\", Pipeline([('clean', CleanText()), \n",
    "                                                               ('tfidf', TfidfVectorizer())])),\n",
    "                                           (\"custom\", CustomFeatures())])),\n",
    "                 (\"select\", SelectKBest(score_func = chi2)),\n",
    "                 (\"scale\", StandardScaler(with_mean = False))]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifier implemented through pipelines: Logistic Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "pipe_logistic = Pipeline([('pre_process', pipe),\n",
    "                          ('classify', LogisticRegression(max_iter=10000, tol=0.1, solver='lbfgs'))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit on training\n",
    "pipe_logistic.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test\n",
    "# The F1 score can be interpreted as a weighted average of the precision and recall, \n",
    "# where an F1 score reaches its best value at 1 and worst score at 0. The relative contribution of precision \n",
    "# and recall to the F1 score are equal. The formula for the F1 score is:\n",
    "# F1 = 2 * (precision * recall) / (precision + recall)\n",
    "from sklearn.metrics import f1_score\n",
    "y_pred = pipe_logistic.predict(X_test)\n",
    "f1_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can classify new messages!\n",
    "msg = pd.DataFrame(columns = [\"text\"],\n",
    "                   data    = [\"The bikes are heavy and unwieldy. The collection and return of the bicycle is super-comfortable because the bikes are heavy\"])\n",
    "\n",
    "pipe_logistic.predict(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can classify new messages!\n",
    "#msg = pd.DataFrame(columns = [\"text\"],\n",
    "                   #data    = [\"REMINDER FROM O2: To get 2.50 pounds free call credit and details of great offers pls reply 2 this text with your valid name, house no and postcode\"])\n",
    "\n",
    "msg = pd.DataFrame(columns = [\"text\"],\n",
    "                   data    = [\"Satisfied\"])\n",
    "\n",
    "\n",
    "pipe_logistic.predict(msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using bi-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract features\n",
    "pipe_extract = FeatureUnion([(\"terms\", Pipeline([('clean', CleanText()), \n",
    "                                                 ('tfidf', TfidfVectorizer())])),\n",
    "                             (\"custom\", CustomFeatures())])\n",
    "\n",
    "# select and scale features\n",
    "pipe_select_scale = Pipeline([(\"select\", SelectKBest(score_func = chi2)),\n",
    "                              (\"scale\", StandardScaler(with_mean = False))]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract features\n",
    "# you can also use bi-grams:\n",
    "X_extract = pipe_extract.set_params(terms__tfidf__ngram_range = (1,2)).fit_transform(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_extract)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract all features\n",
    "X_select_scale = pipe_select_scale.set_params(select__k = 500).fit_transform(X_extract, y_train)\n",
    "print(X_select_scale)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using cross-validation with parameters (grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select best hyperparameters by cross validation\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Model\n",
    "logistic = LogisticRegression(max_iter=10000, tol=0.1, solver='lbfgs')\n",
    "\n",
    "# Parameters: (np.logspace returns numbers spaced evenly on a log scale.)\n",
    "param_logistic = {\n",
    "    'C': np.logspace(-4, 4, 4)\n",
    "}\n",
    "\n",
    "# For an explanation of the 'C' parameter in scikit-learn logistic regression see:\n",
    "# https://stackoverflow.com/questions/22851316/what-is-the-inverse-of-regularization-strength-in-logistic-regression-how-shoul\n",
    "# C= 1/\\lambda where \\lambda can be assimilated to the regulatization parameter\n",
    "# you probably have seen in the lasso regression\n",
    "# Grid Search\n",
    "cv_logistic = GridSearchCV(logistic, param_logistic, cv=10, scoring='f1')\n",
    "cv_logistic.fit(X_select_scale, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html\n",
    "print(cv_logistic.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cv_logistic.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similar with pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipe Logistic\n",
    "pipe_logistic = Pipeline([('select_scale', pipe_select_scale),\n",
    "                          ('classify', LogisticRegression(max_iter=10000, tol=0.1, solver='lbfgs'))])\n",
    "\n",
    "# Parameters of pipelines can be set using ‘__’ separated parameter names:\n",
    "param_logistic = {\n",
    "    'classify__C': np.logspace(-4, 4, 3),\n",
    "    'select_scale__select__k': [600, 1000, 5000]\n",
    "}\n",
    "\n",
    "cv_logistic = GridSearchCV(pipe_logistic, param_logistic, cv=10, scoring='f1')\n",
    "cv_logistic.fit(X_extract, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cv_logistic.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cv_logistic.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# Pipe NB\n",
    "pipe_nb = Pipeline([('select_scale', pipe_select_scale),\n",
    "                    ('classify', MultinomialNB())])\n",
    "\n",
    "# Parameters of pipelines can be set using ‘__’ separated parameter names:\n",
    "param_nb = {\n",
    "    'classify__alpha': [0.5, 1, 10],\n",
    "    'select_scale__select__k': [600, 1000, 5000]\n",
    "}\n",
    "\n",
    "cv_nb = GridSearchCV(pipe_nb, param_nb, cv=10, scoring='f1')\n",
    "cv_nb.fit(X_extract, y_train)\n",
    "print(cv_nb.best_score_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# full pipeline\n",
    "model = Pipeline([(\"extract\", FeatureUnion([(\"terms\", Pipeline([('clean', CleanText()), \n",
    "                                                                ('tfidf', TfidfVectorizer(ngram_range = (1,2)))])),\n",
    "                                           (\"custom\", CustomFeatures())])),\n",
    "                 (\"select\", SelectKBest(score_func = chi2, k = 1000)),\n",
    "                 (\"scale\", StandardScaler(with_mean = False)),\n",
    "                 (\"classify\", MultinomialNB())]) \n",
    "\n",
    "# fitting\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# final evaluation\n",
    "y_pred = model.predict(X_test)\n",
    "f1_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we are now able to classify new messages!\n",
    "#msg = pd.DataFrame(columns = [\"text\"],\n",
    "                   #data    = [\"REMINDER FROM O2: To get 2.50 pounds free call credit and details of great offers pls reply 2 this text with your valid name, house no and postcode\"])\n",
    "\n",
    "msg = pd.DataFrame(columns = [\"text\"],\n",
    "                   data    = [\"The bikes are heavy and unwieldy. The collection and return of the bicycle is super-comfortable because the bikes are heavy\"])\n",
    "\n",
    "model.predict(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we are now able to classify new messages!\n",
    "#msg = pd.DataFrame(columns = [\"text\"],\n",
    "                   #data    = [\"REMINDER FROM O2: To get 2.50 pounds free call credit and details of great offers pls reply 2 this text with your valid name, house no and postcode\"])\n",
    "\n",
    "msg = pd.DataFrame(columns = [\"text\"],\n",
    "                   data    = [\"Satisfied\"])\n",
    "\n",
    "\n",
    "model.predict(msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "# Pipe SVC\n",
    "pipe_svc = Pipeline([('select_scale', pipe_select_scale),\n",
    "                     ('classify', LinearSVC(max_iter=10000, tol=0.1))])\n",
    "\n",
    "# Parameters of pipelines can be set using ‘__’ separated parameter names:\n",
    "param_svc = {\n",
    "    'classify__C': [0.01, 0.1, 1],\n",
    "    'select_scale__select__k': [600, 1000, 5000]\n",
    "}\n",
    "\n",
    "cv_svc = GridSearchCV(pipe_svc, param_svc, cv=10, scoring='f1')\n",
    "cv_svc.fit(X_extract, y_train)\n",
    "print(cv_svc.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# full pipeline\n",
    "model = Pipeline([(\"extract\", FeatureUnion([(\"terms\", Pipeline([('clean', CleanText()), \n",
    "                                                                ('tfidf', TfidfVectorizer(ngram_range = (1,2)))])),\n",
    "                                           (\"custom\", CustomFeatures())])),\n",
    "                 (\"select\", SelectKBest(score_func = chi2, k = 1000)),\n",
    "                 (\"scale\", StandardScaler(with_mean = False)),\n",
    "                 (\"classify\", LinearSVC(C = 1, max_iter=10000, tol=0.1))]) \n",
    "\n",
    "# fitting\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# final evaluation\n",
    "y_pred = model.predict(X_test)\n",
    "f1_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we are now able to classify new messages!\n",
    "#msg = pd.DataFrame(columns = [\"text\"],\n",
    "                   #data    = [\"REMINDER FROM O2: To get 2.50 pounds free call credit and details of great offers pls reply 2 this text with your valid name, house no and postcode\"])\n",
    "\n",
    "msg = pd.DataFrame(columns = [\"text\"],\n",
    "                   data    = [\"The bikes are heavy and unwieldy. The collection and return of the bicycle is super-comfortable because the bikes are heavy\"])\n",
    "\n",
    "model.predict(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we are now able to classify new messages!\n",
    "#msg = pd.DataFrame(columns = [\"text\"],\n",
    "                   #data    = [\"REMINDER FROM O2: To get 2.50 pounds free call credit and details of great offers pls reply 2 this text with your valid name, house no and postcode\"])\n",
    "\n",
    "msg = pd.DataFrame(columns = [\"text\"],\n",
    "                   data    = [\"Satisfied\"])\n",
    "\n",
    "\n",
    "model.predict(msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Pipe RF\n",
    "pipe_rf = Pipeline([('select_scale', pipe_select_scale),\n",
    "                    ('classify', RandomForestClassifier())])\n",
    "\n",
    "# Parameters of pipelines can be set using ‘__’ separated parameter names:\n",
    "param_rf = {\n",
    "    'classify__n_estimators': [100, 200],\n",
    "    'select_scale__select__k': [600, 1000]\n",
    "}\n",
    "\n",
    "cv_rf = GridSearchCV(pipe_rf, param_rf, cv=10, scoring='f1')\n",
    "cv_rf.fit(X_extract, y_train)\n",
    "print(cv_rf.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# full pipeline\n",
    "model = Pipeline([(\"extract\", FeatureUnion([(\"terms\", Pipeline([('clean', CleanText()), \n",
    "                                                                ('tfidf', TfidfVectorizer(ngram_range = (1,2)))])),\n",
    "                                           (\"custom\", CustomFeatures())])),\n",
    "                 (\"select\", SelectKBest(score_func = chi2, k = 1000)),\n",
    "                 (\"scale\", StandardScaler(with_mean = False)),\n",
    "                 (\"classify\", RandomForestClassifier())]) \n",
    "\n",
    "# fitting\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# final evaluation\n",
    "y_pred = model.predict(X_test)\n",
    "f1_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we are now able to classify new messages!\n",
    "#msg = pd.DataFrame(columns = [\"text\"],\n",
    "                   #data    = [\"REMINDER FROM O2: To get 2.50 pounds free call credit and details of great offers pls reply 2 this text with your valid name, house no and postcode\"])\n",
    "\n",
    "msg = pd.DataFrame(columns = [\"text\"],\n",
    "                   data    = [\"The bikes are heavy and unwieldy. The collection and return of the bicycle is super-comfortable because the bikes are heavy\"])\n",
    "\n",
    "model.predict(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we are now able to classify new messages!\n",
    "#msg = pd.DataFrame(columns = [\"text\"],\n",
    "                   #data    = [\"REMINDER FROM O2: To get 2.50 pounds free call credit and details of great offers pls reply 2 this text with your valid name, house no and postcode\"])\n",
    "\n",
    "msg = pd.DataFrame(columns = [\"text\"],\n",
    "                   data    = [\"Satisfied\"])\n",
    "\n",
    "\n",
    "model.predict(msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Long Example 1: Text clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import AgglomerativeClustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full text for clustering\n",
    "\n",
    "This corpus contain some strings about Google and some strings about TF-IDF from Wikipedia. Just for example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_text = \"\"\"\n",
    "Google and Facebook are strangling the free press to death. Democracy is the loser\n",
    "Your 60-second guide to security stuff Google touted today at Next '18\n",
    "A Guide to Using Android Without Selling Your Soul to Google\n",
    "Review: Lenovo’s Google Smart Display is pretty and intelligent\n",
    "Google Maps user spots mysterious object submerged off the coast of Greece - and no-one knows what it is\n",
    "Android is better than IOS\n",
    "In information retrieval, tf–idf or TFIDF, short for term frequency–inverse document frequency\n",
    "is a numerical statistic that is intended to reflect how important \n",
    "a word is to a document in a collection or corpus.\n",
    "It is often used as a weighting factor in searches of information retrieval\n",
    "text mining, and user modeling. The tf-idf value increases proportionally\n",
    "to the number of times a word appears in the document\n",
    "and is offset by the frequency of the word in the corpus\n",
    "\"\"\".split(\"\\n\")[1:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing and tokenizing\n",
    "Firstly, we must bring every chars to lowercase and remove all punctuation, because it's not important for our task, but is very harmful for clustering algorithm. \n",
    "After that, we'll split strings to array of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(line):\n",
    "    line = line.lower()\n",
    "    line = re.sub(r\"[{}]\".format(string.punctuation), \" \", line)\n",
    "    return line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's calculate tf-idf for this corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(preprocessor=preprocessing)\n",
    "tfidf = tfidf_vectorizer.fit_transform(all_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(zip(kmeans.fit_predict(tfidf), all_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agglomerative Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hac = AgglomerativeClustering(n_clusters=2, affinity='cosine', linkage='average')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(zip(hac.fit_predict(tfidf.toarray()), all_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Example 2: Topic model (1): BikeMi survey\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning and pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import string\n",
    "stop=set(stopwords.words('english'))\n",
    "exclude=set(string.punctuation)\n",
    "lemma=WordNetLemmatizer()\n",
    "def clean(doc):\n",
    "    stop_free=\" \".join([i for i in doc.lower().split() if i not in stop])\n",
    "    punc_free=''.join(ch for ch in stop_free if ch not in exclude)\n",
    "    normalized=\" \".join(lemma.lemmatize(word) for word in punc_free.split())\n",
    "    return normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('Polarity2014Reduced.csv', sep = \";\", header = 0)\n",
    "df.columns=['review','sentiment']\n",
    "df2=df[df['sentiment']==-1]\n",
    "df2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_complete=df2.iloc[0:2065,0].values.tolist()\n",
    "doc_clean=[clean(doc).split() for doc in doc_complete]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting the document-term matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    "SOME_FIXED_SEED = 42\n",
    "np.random.seed(SOME_FIXED_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv=CountVectorizer(min_df=2,max_df=50,ngram_range=(1,2), token_pattern=None, tokenizer=lambda doc:doc,preprocessor=lambda doc:doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_features=cv.fit_transform(doc_clean)\n",
    "print(cv_features.shape)\n",
    "vocabulary=np.array(cv.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA ANALYSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using sklearn.decomposition LDA with 11 topics\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "TOTAL_TOPICS=11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model=LatentDirichletAllocation(n_components=TOTAL_TOPICS,max_iter=500,max_doc_update_iter=50,learning_method='online',batch_size=1740,learning_offset=50.,random_state=42,n_jobs=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Using the transformer 'fit_transform'\n",
    "document_topics=lda_model.fit_transform(cv_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_topics.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extraqcting the most important 10 terms for each topic\n",
    "topic_terms=lda_model.components_\n",
    "top_terms=10 # number of 'top terms'\n",
    "topic_key_terms_idxs=np.argsort(-np.absolute(topic_terms), axis=1)[:,:top_terms]\n",
    "topic_keyterms=vocabulary[topic_key_terms_idxs]\n",
    "topics=[', '.join(topic) for topic in topic_keyterms]\n",
    "pd.set_option('display.max_colwidth',-1)\n",
    "topics_df=pd.DataFrame(topics,columns=['Term per Topic'], index=['Topic'+str(t) for t in range(1,TOTAL_TOPICS+1)])\n",
    "topics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_df=pd.DataFrame(document_topics,columns=['T'+str(i) for i in range(1,TOTAL_TOPICS+1)])\n",
    "dt_df                                                                      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column 'Contribution%' gives the max probability among the 354 \n",
    "# features (terms) for each topic\n",
    "dt_df=pd.DataFrame(document_topics,columns=['T'+str(i) for i in range(1,TOTAL_TOPICS+1)])\n",
    "pd.options.display.float_format='{:,.5f}'.format\n",
    "pd.set_option('display.max_colwidth',200)\n",
    "max_contrib_topics=dt_df.max(axis=0)\n",
    "dominant_topics=max_contrib_topics.index\n",
    "contrib_perc=max_contrib_topics.values\n",
    "document_numbers=[dt_df[dt_df[t]==max_contrib_topics.loc[t]].index[0] for t in dominant_topics]\n",
    "results_df=pd.DataFrame({'Dominant Topic':dominant_topics,'Contribution%':contrib_perc, 'Answer Num': document_numbers,'Topic':topics_df['Term per Topic']})\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This gives, for each topic, the % of features having prob >0.9\n",
    "numT1=np.count_nonzero(dt_df['T1']>0.9)\n",
    "FrT1=numT1/2133\n",
    "numT2=np.count_nonzero(dt_df['T2']>0.9)\n",
    "FrT2=numT2/2133\n",
    "numT3=np.count_nonzero(dt_df['T3']>0.9)\n",
    "FrT3=numT3/2133\n",
    "numT4=np.count_nonzero(dt_df['T4']>0.9)\n",
    "FrT4=numT4/2133\n",
    "numT5=np.count_nonzero(dt_df['T5']>0.9)\n",
    "FrT5=numT5/2133\n",
    "numT6=np.count_nonzero(dt_df['T6']>0.9)\n",
    "FrT6=numT6/2133\n",
    "numT7=np.count_nonzero(dt_df['T7']>0.9)\n",
    "FrT7=numT7/2133\n",
    "numT8=np.count_nonzero(dt_df['T8']>0.9)\n",
    "FrT8=numT8/2133\n",
    "numT9=np.count_nonzero(dt_df['T9']>0.9)\n",
    "FrT9=numT9/2133\n",
    "numT10=np.count_nonzero(dt_df['T10']>0.9)\n",
    "FrT10=numT10/2133\n",
    "numT11=np.count_nonzero(dt_df['T11']>0.9)\n",
    "FrT11=numT11/2133\n",
    "d=(FrT1,FrT2,FrT3,FrT4,FrT5,FrT6,FrT7,FrT8,FrT9,FrT10,FrT11)\n",
    "df_Fr=pd.DataFrame(data=d)\n",
    "results_df.insert(4,'Freq 0.9-1',df_Fr.values)\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "EnvTextMining",
   "language": "python",
   "name": "envtextmining"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
