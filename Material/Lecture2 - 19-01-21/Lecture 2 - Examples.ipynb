{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 0: stemmers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk as nltk\n",
    "#PorterStemmer:\n",
    "porter = nltk.PorterStemmer() \n",
    "porter.stem('Manufacturing')#not so good! We should have had \"manufact\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "porter.stem('haved') #not so good!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A problem:\n",
    "porter.stem('relies')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exceptions in grammar:\n",
    "porter.stem('mice')\n",
    "#bad performance! But this problem is much more related to lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Another example:\n",
    "porter.stem('geese')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "porter = nltk.LancasterStemmer() \n",
    "porter.stem('manufacturing') #good!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "porter.stem('haved') #good!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exceptions in grammar:\n",
    "porter.stem('mice')\n",
    "#bad performance! But this problem is much more related to lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Another example:\n",
    "porter.stem('geese')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 1: lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = nltk.WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lemmatizing an adjective:\n",
    "lemmatizer.lemmatize('stricter')# bad performance!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#But let's use Wordnet:\n",
    "lemmatizer.lemmatize('stricter', pos = nltk.corpus.wordnet.ADJ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lemmatizing a noun:\n",
    "lemmatizer.lemmatize('mice')# good performance!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lemmatize as adverb\n",
    "lemmatizer.lemmatize('better', pos = nltk.corpus.wordnet.ADV) #good performance!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 2: POS classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk \n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "#From 'Gone with the wind'\n",
    "txt = \"Frankly, my dear, I don't give a damn!\" \n",
    "nltk.pos_tag(nltk.word_tokenize(txt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 17: creating a grammar and then chunking#Other languages: Russian\n",
    "nltk.download('averaged_perceptron_tagger_ru')\n",
    "nltk.pos_tag(nltk.word_tokenize(\"Илья оторопел и дважды перечитал бумажку.\"), lang='rus')  \n",
    "#\"Ilia' was astonished and twice read the notice\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 3: creating a grammar and then chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grammar1 = ('''NP: {<DT>?<JJ>*<NN>} ''')\n",
    "grammar2 = ('''V: {<VB\\w?>} ''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk \n",
    "from nltk import  RegexpParser\n",
    "text = \"This is a simple example of chuncking a sentence\"\n",
    "tagged = nltk.pos_tag(nltk.word_tokenize(text))\n",
    "tree = nltk.RegexpParser(grammar1).parse(tagged)\n",
    "for subtree in tree.subtrees():\n",
    "    print(subtree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree2 = nltk.RegexpParser(grammar2).parse(tagged)\n",
    "for subtree in tree2.subtrees():\n",
    "    print(subtree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import  RegexpParser\n",
    "# From \"The Guardian\", 11 gen 2021:\n",
    "text = \"With a government this bad in charge of the UK during Covid, how do we respond?\" \n",
    "sentence = nltk.pos_tag(nltk.word_tokenize(text))\n",
    "sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "#sentence = [(\"the\", \"DT\"),(\"book\", \"NN\"),(\"has\",\"VBZ\"),(\"many\",\"JJ\"),(\"chapters\",\"NNS\")]\n",
    "chunker=nltk.RegexpParser(r'''\n",
    "NP:{<DT><NN.*><.*>*<NN.*>}\n",
    "}<VB.*>{\n",
    "''')\n",
    "chunker.parse(sentence)\n",
    "Output=chunker.parse(sentence)\n",
    "Output.draw()\n",
    "#Recall to close the draw window to end execution of the cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 4: named entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "text = \"European authorities fined Google a record 5.1 billion dollars on Wednesday for abusing its power...\"\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "nltk.ne_chunk(nltk.pos_tag(nltk.word_tokenize(text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import spacy\n",
    "#nlp = spacy.load(\"en_core_web_sm\") \n",
    "#doc = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 4: named entitiesfor ent in doc.ents: \n",
    "#print(doc.text, doc.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 5: Regex and text data in Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "opinion = pd.read_csv('BikeMiSurvey_short2.csv', sep = \";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opinion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = pd.DataFrame(opinion['English'])\n",
    "texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the number of characters for each string in texts['English']\n",
    "texts['English'].str.len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the number of tokens for each string in df['text']\n",
    "texts['English'].str.split().str.len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find which entries contain the word 'bike'\n",
    "texts['English'].str.contains('bike')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find how many times a digit occurs in each string (found only number 2 in first row and the time numbers in sixth)\n",
    "texts['English'].str.count(r'\\d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find all occurences of the digits (only 2 in first row and the time numbers in fifth)\n",
    "texts['English'].str.findall(r'\\d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group and find the hours and minutes\n",
    "texts['English'].str.findall(r'(\\d?\\d):(\\d\\d)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace 'Yesterday' and 'Monday' with '???'\n",
    "texts['English'].str.replace(r'\\w+day\\b', '???')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace 'Monday' with 'the first day of the week'\n",
    "sixth_row = pd.DataFrame(texts['English'].str.replace(r'Monday', 'the first day of the week'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sixth_row['English'].iloc[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace weekdays with 3 letter abbrevations (lambda represents an anonymous: If it is used with \n",
    "# in a df\n",
    "#  each element of a series is fed into the lambda function)\n",
    "# Be careful with cases like here where we have Yester-day and Mon-day\n",
    "texts['English'].str.replace(r'(\\w+day\\b)', lambda x: x.groups()[0][:3])\n",
    "#texts['English'].str.replace(r'(\\w+nday\\b)', lambda x: x.groups()[0][:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "trips = pd.read_csv('BIKEMI_TRIPS.csv', sep = \";\")\n",
    "trips.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = pd.DataFrame(trips['CHECK_IN_TIME'])\n",
    "# group and find the hours and minutes\n",
    "texts['CHECK_IN_TIME'].str.findall(r'(\\d?\\d):(\\d\\d)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new columns from first match of extracted groups\n",
    "only_hour = pd.DataFrame(texts['CHECK_IN_TIME'].str.extract(r'(\\d?\\d):(\\d\\d)'))\n",
    "only_hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the entire time, the hours, the minutes, and the period\n",
    "#df['text'].str.extractall(r'((\\d?\\d):(\\d\\d) ?([ap]m))')\n",
    "texts['CHECK_IN_TIME'].str.extractall(r'((\\d?\\d):(\\d\\d))')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trips['CHECK_IN_TIME'] = pd.to_datetime(trips['CHECK_IN_TIME'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trips.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trips['CHECK_IN_DATE_ONLY'] = [d.date() for d in trips['CHECK_IN_TIME']]\n",
    "trips['CHECK_IN_TIME_ONLY'] = [d.time() for d in trips['CHECK_IN_TIME']]\n",
    "trips"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Example 6: bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = corpus = [\n",
    "    'Donald Trump is expected to issue more than 100 presidential pardons.',\n",
    "    'Trump is expected to end his time in office.',\n",
    "    'US defense officials say they are worried about an insider attack.',\n",
    "    'He would like to take the extraordinary step of issuing a pardon for himself']\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "print(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This time we use 2-grams:\n",
    "vectorizer2 = CountVectorizer(analyzer='word', ngram_range=(2, 2))\n",
    "X2 = vectorizer2.fit_transform(corpus)\n",
    "print(vectorizer2.get_feature_names())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X2.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Example 7: TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "print(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(X.toarray())# Example 8: Practice with SpaCy.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Example 8: Practice with SpaCy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SpaCy\n",
    "\n",
    "spaCy is an open-source software library for advanced natural language processing: https://spacy.io/\n",
    "\n",
    "The following code is based on: https://medium.com/@ageitgey/natural-language-processing-is-fun-9a0bff37854e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import textacy.extract\n",
    "from urllib import request\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Load the large English NLP model\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting Facts from text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_facts(keyword, url):\n",
    "\n",
    "    # fetch url\n",
    "    response = request.urlopen(url)\n",
    "    \n",
    "    # read html in utf8\n",
    "    html = response.read().decode('utf8')\n",
    "    \n",
    "    # strip html and get raw text\n",
    "    raw = BeautifulSoup(html, 'html.parser').get_text()\n",
    "    \n",
    "    # you should do some pre-processing...\n",
    "    text = raw.replace('\\n',' ')\n",
    "    \n",
    "    # Parse the document with spaCy\n",
    "    doc = nlp(text)\n",
    "\n",
    "    # Extract semi-structured statements\n",
    "    statements = textacy.extract.semistructured_statements(doc, keyword)\n",
    "\n",
    "    # Print the results\n",
    "    if keyword == 'Biden':\n",
    "        print(\"Here are the things I know about Biden:\\n\")\n",
    "        for statement in statements:\n",
    "            subject, verb, fact = statement\n",
    "            print(f\" - {fact}\")\n",
    "    else:\n",
    "        print(\"Here are the things I know about Trump:\\n\")\n",
    "        for statement in statements:\n",
    "            subject, verb, fact = statement\n",
    "            print(f\" - {fact}\")\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print facts about London fetching the wikipedia page\n",
    "print_facts(\"Biden\", \"https://en.wikipedia.org/wiki/Joe_Biden\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print facts about Trump fetching the wikipedia page\n",
    "print_facts(\"Trump\", \"https://en.wikipedia.org/wiki/Donald_Trump\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What else can we do?\n",
    "\n",
    "Imagine that you were building a website that let’s the user view information for every city in the world using the information we extracted in the last example. If you had a search feature on the website, it might be nice to __autocomplete__ common search queries like Google does. But to do this, we need a list of possible completions to suggest to the user. We can use NLP to quickly generate this data. Here’s one way to extract frequently-mentioned noun chunks from a document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def autocomplete(keyword, url, min_freq):\n",
    "    \n",
    "    # fetch url\n",
    "    response = request.urlopen(url)\n",
    "    \n",
    "    # read html in utf8\n",
    "    html = response.read().decode('utf8')\n",
    "    \n",
    "    # strip html and get raw text\n",
    "    raw = BeautifulSoup(html, 'html.parser').get_text()\n",
    "    \n",
    "    # you should do some pre-processing...\n",
    "    text = raw.replace('\\n',' ')\n",
    "    \n",
    "    # Parse the document with spaCy\n",
    "    doc = nlp(text)\n",
    "\n",
    "    # Extract noun chunks that appear\n",
    "    noun_chunks = textacy.extract.noun_chunks(doc, min_freq = min_freq)\n",
    "\n",
    "    # Convert noun chunks to lowercase strings\n",
    "    noun_chunks = map(str, noun_chunks)\n",
    "    noun_chunks = map(str.lower, noun_chunks)\n",
    "\n",
    "    # Collect any nouns that are at least 2 words long\n",
    "    res = []\n",
    "    for noun_chunk in set(noun_chunks):\n",
    "        if len(noun_chunk.split(\" \")) > 1:\n",
    "            res.append(noun_chunk)\n",
    "        \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# autocomplete Biden\n",
    "autocomplete(\"Biden\", \"https://en.wikipedia.org/wiki/Joe_Biden\", 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# autocomplete Trump\n",
    "autocomplete(\"Trump\", \"https://en.wikipedia.org/wiki/Donald_Trump\", 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 9: PCA in text mining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def vectorizing(data):\n",
    "    vec = CountVectorizer()\n",
    "    X = vec.fit_transform(data)\n",
    "    df = pd.DataFrame(X.toarray(), columns=vec.get_feature_names())\n",
    "    return df\n",
    "\n",
    "def find_principal_components(n, data):\n",
    "    pca = PCA(n_components = n)\n",
    "    principalComponents = pca.fit_transform(data)\n",
    "    return pd.DataFrame(pca.components_, columns=data.columns)\n",
    "\n",
    "text = ['Texas real estate agent Ryan Williams',\n",
    "        'part mob Trump storm administration Capitol congress continue insist innocence',\n",
    "        'even face charge breach Capitol guilt heart Ryan', \n",
    "        'tell today Pelosi show glad Ryan Williams there because witness history Trump administration', \n",
    "        'never get  chance do again Texas Capitol there mob', \n",
    "        'storm Pelosi laptop invade office congress Pelosi Trump Biden prison guilt prison breach steal laptop',\n",
    "        'Trump Williams Biden Trump president elect president Trump']\n",
    "\n",
    "df = vectorizing(text)\n",
    "\n",
    "print(df) # 7 row x 44 columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "principalDF = find_principal_components(2, df)\n",
    "\n",
    "print(principalDF) # 2 rows x 44 columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "EnvTextMining",
   "language": "python",
   "name": "envtextmining"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
