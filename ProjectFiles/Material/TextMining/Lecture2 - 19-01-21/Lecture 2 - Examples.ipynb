{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "EnvTextMining",
      "language": "python",
      "name": "envtextmining"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "Lecture 2 - Examples.ipynb",
      "provenance": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w7vDcxlFAnKs"
      },
      "source": [
        "# Example 0: stemmers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KhgpwE_XAnLB",
        "outputId": "13c98116-e2e0-4ead-c94f-2c9c983a9ea7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "import nltk as nltk\n",
        "#PorterStemmer:\n",
        "porter = nltk.PorterStemmer() \n",
        "porter.stem('Manufacturing')#not so good! We should have had \"manufact\"."
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'manufactur'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O06CAYpvAnLD",
        "outputId": "3fffdebe-e768-4323-f88f-5901158b039e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "porter.stem('haved') #not so good!"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'have'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UPYoXzLpAnLE",
        "outputId": "3c2d7d40-91b3-489b-f9f2-db129f3da865",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "#A problem:\n",
        "porter.stem('relies')"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'reli'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YcPPnNsnAnLO",
        "outputId": "7b1cf5d7-ad4e-4aa6-854f-25e719c11d5b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "#Exceptions in grammar:\n",
        "porter.stem('mice')\n",
        "#bad performance! But this problem is much more related to lemmatization"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'mice'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "irFCFhszAnLP",
        "outputId": "840368bd-71fe-41b8-e8cf-3dd765c17568",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "#Another example:\n",
        "porter.stem('geese')"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'gees'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MhW-GT-dAnLQ",
        "outputId": "9365e532-faee-4fa5-e0d6-c7bd0d3cd5ef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "porter = nltk.LancasterStemmer() \n",
        "porter.stem('manufacturing') #good!"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'manufact'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GaMNDh-iAnLS",
        "outputId": "6ff6b28d-c669-4cae-ff50-4ae7677e36e9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "porter.stem('haved') #good!"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'hav'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pyZEKsGPAnLS",
        "outputId": "4dc1d6e0-e1d9-4fc8-f344-25187d8ff3fb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "#Exceptions in grammar:\n",
        "porter.stem('mice')\n",
        "#bad performance! But this problem is much more related to lemmatization"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'mic'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n_oeQmVOAnLT",
        "outputId": "dd1bd2e3-e39f-468f-d357-29e02c77b744",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "#Another example:\n",
        "porter.stem('geese')"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'gees'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5AglMDmTAnLT"
      },
      "source": [
        "# Example 1: lemmatization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C32sU-DbAnLV"
      },
      "source": [
        "lemmatizer = nltk.WordNetLemmatizer()"
      ],
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "voS4APuBAnLf",
        "outputId": "21e0a0e6-7c49-43f2-9cfa-ef0cfabde4f9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "#lemmatizing an adjective:\n",
        "nltk.download('wordnet')\n",
        "lemmatizer.lemmatize('stricter')# bad performance!"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'stricter'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IitTaEl7AnLg",
        "outputId": "3d130afd-746a-4e27-8181-9fb497d46382",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "#But let's use Wordnet:\n",
        "lemmatizer.lemmatize('stricter', pos = nltk.corpus.wordnet.ADJ)"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'strict'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "avzBTxXjAnLg",
        "outputId": "3a6273c2-f860-4e74-f183-474a008c3b4b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "#lemmatizing a noun:\n",
        "lemmatizer.lemmatize('mice')# good performance!"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'mouse'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JYFjmARpAnLh",
        "outputId": "9404a413-9b30-4269-8c99-b170bc81da59",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "# lemmatize as adverb\n",
        "lemmatizer.lemmatize('better', pos = nltk.corpus.wordnet.ADV) #good performance!"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'well'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RLfwyG2EAnLh"
      },
      "source": [
        "# Example 2: POS classification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GGQ7J4epAnLh",
        "outputId": "83cb2acb-e54d-4203-9f64-3c123a7d0db9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import nltk \n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('punkt')\n",
        "#From 'Gone with the wind'\n",
        "txt = \"Frankly, my dear, I don't give a damn!\" \n",
        "nltk.pos_tag(nltk.word_tokenize(txt))"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Frankly', 'RB'),\n",
              " (',', ','),\n",
              " ('my', 'PRP$'),\n",
              " ('dear', 'JJ'),\n",
              " (',', ','),\n",
              " ('I', 'PRP'),\n",
              " ('do', 'VBP'),\n",
              " (\"n't\", 'RB'),\n",
              " ('give', 'VB'),\n",
              " ('a', 'DT'),\n",
              " ('damn', 'NN'),\n",
              " ('!', '.')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ncnlx6HAnLi",
        "outputId": "a601a737-53c0-432c-f465-02a743e317d5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Example 17: creating a grammar and then chunking#Other languages: Russian\n",
        "nltk.download('averaged_perceptron_tagger_ru')\n",
        "nltk.pos_tag(nltk.word_tokenize(\"Илья оторопел и дважды перечитал бумажку.\"), lang='rus')  \n",
        "#\"Ilia' was astonished and twice read the notice\""
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger_ru to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger_ru is already up-to-\n",
            "[nltk_data]       date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Илья', 'S'),\n",
              " ('оторопел', 'V'),\n",
              " ('и', 'CONJ'),\n",
              " ('дважды', 'ADV'),\n",
              " ('перечитал', 'V'),\n",
              " ('бумажку', 'S'),\n",
              " ('.', 'NONLEX')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l7267wLvAnLi"
      },
      "source": [
        "# Example 3: creating a grammar and then chunking"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RbOCJhjMAnLk"
      },
      "source": [
        "grammar1 = ('''NP: {<DT>?<JJ>*<NN>} ''')\n",
        "grammar2 = ('''V: {<VB\\w?>} ''')"
      ],
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IXy5VIRyAnLk",
        "outputId": "8b104063-817b-49a5-c2c7-4d97d858813f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import nltk \n",
        "from nltk import  RegexpParser\n",
        "text = \"This is a simple example of chuncking a sentence\"\n",
        "tagged = nltk.pos_tag(nltk.word_tokenize(text))\n",
        "tree = nltk.RegexpParser(grammar1).parse(tagged)\n",
        "for subtree in tree.subtrees():\n",
        "    print(subtree)"
      ],
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(S\n",
            "  This/DT\n",
            "  is/VBZ\n",
            "  (NP a/DT simple/JJ example/NN)\n",
            "  of/IN\n",
            "  chuncking/VBG\n",
            "  (NP a/DT sentence/NN))\n",
            "(NP a/DT simple/JJ example/NN)\n",
            "(NP a/DT sentence/NN)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8taNIzRjAnLk",
        "outputId": "0a3736a1-fb00-4744-e968-6f626bcf1aa2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "tree2 = nltk.RegexpParser(grammar2).parse(tagged)\n",
        "for subtree in tree2.subtrees():\n",
        "    print(subtree)"
      ],
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(S\n",
            "  This/DT\n",
            "  (V is/VBZ)\n",
            "  a/DT\n",
            "  simple/JJ\n",
            "  example/NN\n",
            "  of/IN\n",
            "  (V chuncking/VBG)\n",
            "  a/DT\n",
            "  sentence/NN)\n",
            "(V is/VBZ)\n",
            "(V chuncking/VBG)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zxB8yK2KAnLl",
        "outputId": "c566871b-0330-481f-8ec9-cc43479d004a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from nltk import  RegexpParser\n",
        "# From \"The Guardian\", 11 gen 2021:\n",
        "text = \"With a government this bad in charge of the UK during Covid, how do we respond?\" \n",
        "sentence = nltk.pos_tag(nltk.word_tokenize(text))\n",
        "sentence"
      ],
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('With', 'IN'),\n",
              " ('a', 'DT'),\n",
              " ('government', 'NN'),\n",
              " ('this', 'DT'),\n",
              " ('bad', 'JJ'),\n",
              " ('in', 'IN'),\n",
              " ('charge', 'NN'),\n",
              " ('of', 'IN'),\n",
              " ('the', 'DT'),\n",
              " ('UK', 'NNP'),\n",
              " ('during', 'IN'),\n",
              " ('Covid', 'NNP'),\n",
              " (',', ','),\n",
              " ('how', 'WRB'),\n",
              " ('do', 'VBP'),\n",
              " ('we', 'PRP'),\n",
              " ('respond', 'VB'),\n",
              " ('?', '.')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VjtRLRwDAnLl"
      },
      "source": [
        "import nltk\n",
        "#sentence = [(\"the\", \"DT\"),(\"book\", \"NN\"),(\"has\",\"VBZ\"),(\"many\",\"JJ\"),(\"chapters\",\"NNS\")]\n",
        "chunker=nltk.RegexpParser(r'''\n",
        "NP:{<DT><NN.*><.*>*<NN.*>}\n",
        "}<VB.*>{\n",
        "''')\n",
        "chunker.parse(sentence)\n",
        "Output=chunker.parse(sentence)\n",
        "#Output.draw()\n",
        "#Recall to close the draw window to end execution of the cell"
      ],
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EiaCBprwAnLn"
      },
      "source": [
        "# Example 4: named entities"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IrxuVX1CAnLy",
        "outputId": "ba395f8d-8f46-42ef-f43a-323bc62e29e2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 517
        }
      },
      "source": [
        "import nltk\n",
        "text = \"European authorities fined Google a record 5.1 billion dollars on Wednesday for abusing its power...\"\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')\n",
        "nltk.ne_chunk(nltk.pos_tag(nltk.word_tokenize(text)))"
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "TclError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTclError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/core/formatters.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    336\u001b[0m             \u001b[0mmethod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_real_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_method\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    337\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 338\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    339\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nltk/tree.py\u001b[0m in \u001b[0;36m_repr_png_\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    717\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCanvasFrame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    718\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minternals\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfind_binary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 719\u001b[0;31m         \u001b[0m_canvas_frame\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCanvasFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    720\u001b[0m         \u001b[0mwidget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtree_to_treesegment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_canvas_frame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0m_canvas_frame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_widget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwidget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nltk/draw/util.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, parent, **kw)\u001b[0m\n\u001b[1;32m   1651\u001b[0m         \u001b[0;31m# If no parent was given, set up a top-level window.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1652\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mparent\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1653\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1654\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'NLTK'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1655\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'<Control-p>'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_to_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/tkinter/__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, screenName, baseName, className, useTk, sync, use)\u001b[0m\n\u001b[1;32m   2021\u001b[0m                 \u001b[0mbaseName\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbaseName\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2022\u001b[0m         \u001b[0minteractive\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2023\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_tkinter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscreenName\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbaseName\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclassName\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minteractive\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwantobjects\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0museTk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msync\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2024\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0museTk\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2025\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_loadtk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTclError\u001b[0m: no display name and no $DISPLAY environment variable"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Tree('S', [Tree('GPE', [('European', 'JJ')]), ('authorities', 'NNS'), ('fined', 'VBD'), Tree('PERSON', [('Google', 'NNP')]), ('a', 'DT'), ('record', 'NN'), ('5.1', 'CD'), ('billion', 'CD'), ('dollars', 'NNS'), ('on', 'IN'), ('Wednesday', 'NNP'), ('for', 'IN'), ('abusing', 'VBG'), ('its', 'PRP$'), ('power', 'NN'), ('...', ':')])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IfH6DOEIAnLy"
      },
      "source": [
        "#import spacy\n",
        "#nlp = spacy.load(\"en_core_web_sm\") \n",
        "#doc = nlp(text)"
      ],
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pzkccvLeAnLy"
      },
      "source": [
        "# Example 4: named entitiesfor ent in doc.ents: \n",
        "#print(doc.text, doc.label_)"
      ],
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DKfbayBsAnLz"
      },
      "source": [
        "# Example 5: Regex and text data in Pandas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "hTcZJsRHAnLz"
      },
      "source": [
        "import pandas as pd\n",
        "opinion = pd.read_csv('BikeMiSurvey_short2.csv', sep = \";\")"
      ],
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oqwTsG8YAnL0",
        "outputId": "f67bc49d-24a3-480c-e81a-82507b204031",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "source": [
        "opinion"
      ],
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Italian</th>\n",
              "      <th>English</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Quando ho avuto problemi con la restituzione d...</td>\n",
              "      <td>When I had problems with returning the bike, t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>più bici elettriche. spesso anche se presenti ...</td>\n",
              "      <td>more electric bikes. often even if present the...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>fare più attenzione alle stazioni che molto sp...</td>\n",
              "      <td>pay more attention to stations that very often...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>fondamentale inserire bici con seggiolino bimbi</td>\n",
              "      <td>essential to introduce bikes with child seat</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Ieri ho preso la bicicletta alle 7:00 e l'ho l...</td>\n",
              "      <td>Yesterday I took my bike at 7:00 and left it a...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Lunedì sono andato dalla Stazione Centrale al ...</td>\n",
              "      <td>On Monday I went from the Central Station to t...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                             Italian                                            English\n",
              "0  Quando ho avuto problemi con la restituzione d...  When I had problems with returning the bike, t...\n",
              "1  più bici elettriche. spesso anche se presenti ...  more electric bikes. often even if present the...\n",
              "2  fare più attenzione alle stazioni che molto sp...  pay more attention to stations that very often...\n",
              "3    fondamentale inserire bici con seggiolino bimbi       essential to introduce bikes with child seat\n",
              "4  Ieri ho preso la bicicletta alle 7:00 e l'ho l...  Yesterday I took my bike at 7:00 and left it a...\n",
              "5  Lunedì sono andato dalla Stazione Centrale al ...  On Monday I went from the Central Station to t..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 96
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U0TID3RwAnL0",
        "outputId": "4ecae7d9-8ee4-472f-daec-70179d4c61c4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "source": [
        "texts = pd.DataFrame(opinion['English'])\n",
        "texts"
      ],
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>English</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>When I had problems with returning the bike, t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>more electric bikes. often even if present the...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>pay more attention to stations that very often...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>essential to introduce bikes with child seat</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Yesterday I took my bike at 7:00 and left it a...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>On Monday I went from the Central Station to t...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                             English\n",
              "0  When I had problems with returning the bike, t...\n",
              "1  more electric bikes. often even if present the...\n",
              "2  pay more attention to stations that very often...\n",
              "3       essential to introduce bikes with child seat\n",
              "4  Yesterday I took my bike at 7:00 and left it a...\n",
              "5  On Monday I went from the Central Station to t..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 97
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q4Jse3gHAnL0",
        "outputId": "ddc1da5c-9d94-4bfe-d9f6-4e96592ca9fb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# find the number of characters for each string in texts['English']\n",
        "texts['English'].str.len()"
      ],
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    538\n",
              "1     90\n",
              "2    118\n",
              "3     44\n",
              "4    102\n",
              "5    101\n",
              "Name: English, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 98
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OF0SRs3gAnL0",
        "outputId": "bc10854f-f94c-4f13-c662-243a6d77d27f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# find the number of tokens for each string in df['text']\n",
        "texts['English'].str.split().str.len()"
      ],
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    101\n",
              "1     16\n",
              "2     21\n",
              "3      7\n",
              "4     21\n",
              "5     19\n",
              "Name: English, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 99
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sZI1sfwSAnL1",
        "outputId": "59f0add3-e78c-4fe4-fbb9-f8ba9ef0704e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# find which entries contain the word 'bike'\n",
        "texts['English'].str.contains('bike')"
      ],
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0     True\n",
              "1     True\n",
              "2    False\n",
              "3     True\n",
              "4     True\n",
              "5     True\n",
              "Name: English, dtype: bool"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 100
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vjTlmKrjAnL1",
        "outputId": "a16d6e15-8ea2-41b4-a1c1-018dbc962e65",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# find how many times a digit occurs in each string (found only number 2 in first row and the time numbers in sixth)\n",
        "texts['English'].str.count(r'\\d')"
      ],
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    1\n",
              "1    0\n",
              "2    0\n",
              "3    0\n",
              "4    6\n",
              "5    0\n",
              "Name: English, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 101
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YOFrU99BAnL2",
        "outputId": "1d6b23c9-08c1-4b53-9e3a-40aeb0d06ccb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# find all occurences of the digits (only 2 in first row and the time numbers in fifth)\n",
        "texts['English'].str.findall(r'\\d')"
      ],
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0                   [2]\n",
              "1                    []\n",
              "2                    []\n",
              "3                    []\n",
              "4    [7, 0, 0, 7, 2, 9]\n",
              "5                    []\n",
              "Name: English, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 102
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y23d_nLwAnL2",
        "outputId": "c683feec-aef9-41c1-f30e-fb0f51633559",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# group and find the hours and minutes\n",
        "texts['English'].str.findall(r'(\\d?\\d):(\\d\\d)')"
      ],
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0                    []\n",
              "1                    []\n",
              "2                    []\n",
              "3                    []\n",
              "4    [(7, 00), (7, 29)]\n",
              "5                    []\n",
              "Name: English, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 103
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cu-UJDYhAnL2",
        "outputId": "e4d0f518-7221-4d12-bd82-98f6923234e5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# replace 'Yesterday' and 'Monday' with '???'\n",
        "texts['English'].str.replace(r'\\w+day\\b', '???')"
      ],
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    When I had problems with returning the bike, t...\n",
              "1    more electric bikes. often even if present the...\n",
              "2    pay more attention to stations that very often...\n",
              "3         essential to introduce bikes with child seat\n",
              "4    ??? I took my bike at 7:00 and left it at 7:29...\n",
              "5    On ??? I went from the Central Station to the ...\n",
              "Name: English, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 104
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dp5klf_8AnL3"
      },
      "source": [
        "# replace 'Monday' with 'the first day of the week'\n",
        "sixth_row = pd.DataFrame(texts['English'].str.replace(r'Monday', 'the first day of the week'))"
      ],
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rMwHbQqXAnL3",
        "outputId": "511bdd1d-030f-4854-ba20-376b4f8b36e4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "sixth_row['English'].iloc[5]"
      ],
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'On the first day of the week I went from the Central Station to the Duomo and halfway the bike broke. They always break.'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 106
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j3djx8acAnL3",
        "outputId": "5bf628c0-512a-48ab-8e62-9763f6153453",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# replace weekdays with 3 letter abbrevations (lambda represents an anonymous: If it is used with \n",
        "# in a df\n",
        "#  each element of a series is fed into the lambda function)\n",
        "# Be careful with cases like here where we have Yester-day and Mon-day\n",
        "texts['English'].str.replace(r'(\\w+day\\b)', lambda x: x.groups()[0][:3])\n",
        "#texts['English'].str.replace(r'(\\w+nday\\b)', lambda x: x.groups()[0][:3])"
      ],
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    When I had problems with returning the bike, t...\n",
              "1    more electric bikes. often even if present the...\n",
              "2    pay more attention to stations that very often...\n",
              "3         essential to introduce bikes with child seat\n",
              "4    Yes I took my bike at 7:00 and left it at 7:29...\n",
              "5    On Mon I went from the Central Station to the ...\n",
              "Name: English, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 107
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ZA70L39AnL_",
        "outputId": "2cc15439-2aab-4804-8ecb-3faceac0ec62",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 261
        }
      },
      "source": [
        "import pandas as pd\n",
        "trips = pd.read_csv('BIKEMI_TRIPS.csv', sep = \";\")\n",
        "trips.head()"
      ],
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>BIKE_ID</th>\n",
              "      <th>BIKE_TIPE</th>\n",
              "      <th>USER_ID</th>\n",
              "      <th>CHECK_IN_TIME</th>\n",
              "      <th>CHECK_IN_STATION_ID</th>\n",
              "      <th>CHECK_IN_STATION_NAME</th>\n",
              "      <th>CHECK_IN_SLOT_ID</th>\n",
              "      <th>CHECK_OUT_TIME</th>\n",
              "      <th>CHECK_OUT_STATION_ID</th>\n",
              "      <th>CHECK_OUT_STATION_NAME</th>\n",
              "      <th>CHECK_OUT_SLOT_ID</th>\n",
              "      <th>DURATION</th>\n",
              "      <th>TOTAL_DISTANCE</th>\n",
              "      <th>AVOIDED_CO2</th>\n",
              "      <th>CONSUMED_CALORIES</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>20283</td>\n",
              "      <td>Bike</td>\n",
              "      <td>34956</td>\n",
              "      <td>01/01/19 06:52</td>\n",
              "      <td>187</td>\n",
              "      <td>Sarpi Albertini</td>\n",
              "      <td>7</td>\n",
              "      <td>01/01/19 07:05</td>\n",
              "      <td>25</td>\n",
              "      <td>Centrale 1</td>\n",
              "      <td>10</td>\n",
              "      <td>0:13:25</td>\n",
              "      <td>3408.92</td>\n",
              "      <td>0.6913</td>\n",
              "      <td>65.66</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>30131</td>\n",
              "      <td>Child Seat eBike</td>\n",
              "      <td>355600</td>\n",
              "      <td>01/01/19 07:01</td>\n",
              "      <td>302</td>\n",
              "      <td>De Angeli - Ripamonti</td>\n",
              "      <td>9</td>\n",
              "      <td>01/01/19 07:10</td>\n",
              "      <td>23</td>\n",
              "      <td>Regina Margherita</td>\n",
              "      <td>14</td>\n",
              "      <td>0:09:18</td>\n",
              "      <td>1813.18</td>\n",
              "      <td>0.3677</td>\n",
              "      <td>34.93</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>21911</td>\n",
              "      <td>Bike</td>\n",
              "      <td>236069</td>\n",
              "      <td>01/01/19 07:05</td>\n",
              "      <td>222</td>\n",
              "      <td>Durante - D'aviano</td>\n",
              "      <td>22</td>\n",
              "      <td>01/01/19 07:30</td>\n",
              "      <td>110</td>\n",
              "      <td>S. F. Romana</td>\n",
              "      <td>30</td>\n",
              "      <td>0:24:35</td>\n",
              "      <td>2025.83</td>\n",
              "      <td>0.4108</td>\n",
              "      <td>39.02</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>10863</td>\n",
              "      <td>eBike</td>\n",
              "      <td>348357</td>\n",
              "      <td>01/01/19 07:06</td>\n",
              "      <td>257</td>\n",
              "      <td>Valtellina - Aprica</td>\n",
              "      <td>13</td>\n",
              "      <td>01/01/19 08:12</td>\n",
              "      <td>262</td>\n",
              "      <td>Livigno - Monte San Genesio</td>\n",
              "      <td>15</td>\n",
              "      <td>1:06:22</td>\n",
              "      <td>887.54</td>\n",
              "      <td>0.1800</td>\n",
              "      <td>17.10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2781</td>\n",
              "      <td>Bike</td>\n",
              "      <td>147224</td>\n",
              "      <td>01/01/19 07:07</td>\n",
              "      <td>154</td>\n",
              "      <td>Ascanio Sforza - Pavia</td>\n",
              "      <td>6</td>\n",
              "      <td>01/01/19 08:10</td>\n",
              "      <td>64</td>\n",
              "      <td>Diaz</td>\n",
              "      <td>36</td>\n",
              "      <td>1:02:46</td>\n",
              "      <td>2104.10</td>\n",
              "      <td>0.4267</td>\n",
              "      <td>40.53</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   BIKE_ID         BIKE_TIPE  ...  AVOIDED_CO2 CONSUMED_CALORIES\n",
              "0    20283              Bike  ...       0.6913             65.66\n",
              "1    30131  Child Seat eBike  ...       0.3677             34.93\n",
              "2    21911              Bike  ...       0.4108             39.02\n",
              "3    10863             eBike  ...       0.1800             17.10\n",
              "4     2781              Bike  ...       0.4267             40.53\n",
              "\n",
              "[5 rows x 15 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 108
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SkKO3j9KAnL_",
        "outputId": "66a56e2b-e842-4d99-9c94-f7d704a5c140",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "texts = pd.DataFrame(trips['CHECK_IN_TIME'])\n",
        "# group and find the hours and minutes\n",
        "texts['CHECK_IN_TIME'].str.findall(r'(\\d?\\d):(\\d\\d)')"
      ],
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0     [(06, 52)]\n",
              "1     [(07, 01)]\n",
              "2     [(07, 05)]\n",
              "3     [(07, 06)]\n",
              "4     [(07, 07)]\n",
              "5     [(07, 20)]\n",
              "6     [(07, 22)]\n",
              "7     [(07, 26)]\n",
              "8     [(07, 32)]\n",
              "9     [(07, 42)]\n",
              "10    [(07, 43)]\n",
              "11    [(07, 52)]\n",
              "12    [(07, 59)]\n",
              "Name: CHECK_IN_TIME, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 109
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6CDaziq0AnL_",
        "outputId": "237e4ef5-4c85-4f8b-d563-853ad5d7d6a3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 457
        }
      },
      "source": [
        "# create new columns from first match of extracted groups\n",
        "only_hour = pd.DataFrame(texts['CHECK_IN_TIME'].str.extract(r'(\\d?\\d):(\\d\\d)'))\n",
        "only_hour"
      ],
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>06</td>\n",
              "      <td>52</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>07</td>\n",
              "      <td>01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>07</td>\n",
              "      <td>05</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>07</td>\n",
              "      <td>06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>07</td>\n",
              "      <td>07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>07</td>\n",
              "      <td>20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>07</td>\n",
              "      <td>22</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>07</td>\n",
              "      <td>26</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>07</td>\n",
              "      <td>32</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>07</td>\n",
              "      <td>42</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>07</td>\n",
              "      <td>43</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>07</td>\n",
              "      <td>52</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>07</td>\n",
              "      <td>59</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     0   1\n",
              "0   06  52\n",
              "1   07  01\n",
              "2   07  05\n",
              "3   07  06\n",
              "4   07  07\n",
              "5   07  20\n",
              "6   07  22\n",
              "7   07  26\n",
              "8   07  32\n",
              "9   07  42\n",
              "10  07  43\n",
              "11  07  52\n",
              "12  07  59"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 110
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YPnRHLe-AnMB",
        "outputId": "c431bdeb-113c-4000-ffb9-87121d717e09",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 489
        }
      },
      "source": [
        "# extract the entire time, the hours, the minutes, and the period\n",
        "#df['text'].str.extractall(r'((\\d?\\d):(\\d\\d) ?([ap]m))')\n",
        "texts['CHECK_IN_TIME'].str.extractall(r'((\\d?\\d):(\\d\\d))')"
      ],
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th></th>\n",
              "      <th>match</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <th>0</th>\n",
              "      <td>06:52</td>\n",
              "      <td>06</td>\n",
              "      <td>52</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <th>0</th>\n",
              "      <td>07:01</td>\n",
              "      <td>07</td>\n",
              "      <td>01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <th>0</th>\n",
              "      <td>07:05</td>\n",
              "      <td>07</td>\n",
              "      <td>05</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <th>0</th>\n",
              "      <td>07:06</td>\n",
              "      <td>07</td>\n",
              "      <td>06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <th>0</th>\n",
              "      <td>07:07</td>\n",
              "      <td>07</td>\n",
              "      <td>07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <th>0</th>\n",
              "      <td>07:20</td>\n",
              "      <td>07</td>\n",
              "      <td>20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <th>0</th>\n",
              "      <td>07:22</td>\n",
              "      <td>07</td>\n",
              "      <td>22</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <th>0</th>\n",
              "      <td>07:26</td>\n",
              "      <td>07</td>\n",
              "      <td>26</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <th>0</th>\n",
              "      <td>07:32</td>\n",
              "      <td>07</td>\n",
              "      <td>32</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <th>0</th>\n",
              "      <td>07:42</td>\n",
              "      <td>07</td>\n",
              "      <td>42</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <th>0</th>\n",
              "      <td>07:43</td>\n",
              "      <td>07</td>\n",
              "      <td>43</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <th>0</th>\n",
              "      <td>07:52</td>\n",
              "      <td>07</td>\n",
              "      <td>52</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <th>0</th>\n",
              "      <td>07:59</td>\n",
              "      <td>07</td>\n",
              "      <td>59</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "              0   1   2\n",
              "   match               \n",
              "0  0      06:52  06  52\n",
              "1  0      07:01  07  01\n",
              "2  0      07:05  07  05\n",
              "3  0      07:06  07  06\n",
              "4  0      07:07  07  07\n",
              "5  0      07:20  07  20\n",
              "6  0      07:22  07  22\n",
              "7  0      07:26  07  26\n",
              "8  0      07:32  07  32\n",
              "9  0      07:42  07  42\n",
              "10 0      07:43  07  43\n",
              "11 0      07:52  07  52\n",
              "12 0      07:59  07  59"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 111
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-XkTYbuTAnMB"
      },
      "source": [
        "trips['CHECK_IN_TIME'] = pd.to_datetime(trips['CHECK_IN_TIME'])\n"
      ],
      "execution_count": 112,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Lm0USZlAnMB",
        "outputId": "9dee4775-a0a8-4be6-f9bf-d037b39a4ed5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "trips.dtypes"
      ],
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BIKE_ID                            int64\n",
              "BIKE_TIPE                         object\n",
              "USER_ID                            int64\n",
              "CHECK_IN_TIME             datetime64[ns]\n",
              "CHECK_IN_STATION_ID                int64\n",
              "CHECK_IN_STATION_NAME             object\n",
              "CHECK_IN_SLOT_ID                   int64\n",
              "CHECK_OUT_TIME                    object\n",
              "CHECK_OUT_STATION_ID               int64\n",
              "CHECK_OUT_STATION_NAME            object\n",
              "CHECK_OUT_SLOT_ID                  int64\n",
              "DURATION                          object\n",
              "TOTAL_DISTANCE                   float64\n",
              "AVOIDED_CO2                      float64\n",
              "CONSUMED_CALORIES                float64\n",
              "dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 113
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "9rqJ34jXAnMB",
        "outputId": "f176e580-d59e-4246-deb9-7070bc9d9272",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 703
        }
      },
      "source": [
        "trips['CHECK_IN_DATE_ONLY'] = [d.date() for d in trips['CHECK_IN_TIME']]\n",
        "trips['CHECK_IN_TIME_ONLY'] = [d.time() for d in trips['CHECK_IN_TIME']]\n",
        "trips"
      ],
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>BIKE_ID</th>\n",
              "      <th>BIKE_TIPE</th>\n",
              "      <th>USER_ID</th>\n",
              "      <th>CHECK_IN_TIME</th>\n",
              "      <th>CHECK_IN_STATION_ID</th>\n",
              "      <th>CHECK_IN_STATION_NAME</th>\n",
              "      <th>CHECK_IN_SLOT_ID</th>\n",
              "      <th>CHECK_OUT_TIME</th>\n",
              "      <th>CHECK_OUT_STATION_ID</th>\n",
              "      <th>CHECK_OUT_STATION_NAME</th>\n",
              "      <th>CHECK_OUT_SLOT_ID</th>\n",
              "      <th>DURATION</th>\n",
              "      <th>TOTAL_DISTANCE</th>\n",
              "      <th>AVOIDED_CO2</th>\n",
              "      <th>CONSUMED_CALORIES</th>\n",
              "      <th>CHECK_IN_DATE_ONLY</th>\n",
              "      <th>CHECK_IN_TIME_ONLY</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>20283</td>\n",
              "      <td>Bike</td>\n",
              "      <td>34956</td>\n",
              "      <td>2019-01-01 06:52:00</td>\n",
              "      <td>187</td>\n",
              "      <td>Sarpi Albertini</td>\n",
              "      <td>7</td>\n",
              "      <td>01/01/19 07:05</td>\n",
              "      <td>25</td>\n",
              "      <td>Centrale 1</td>\n",
              "      <td>10</td>\n",
              "      <td>0:13:25</td>\n",
              "      <td>3408.92</td>\n",
              "      <td>0.6913</td>\n",
              "      <td>65.66</td>\n",
              "      <td>2019-01-01</td>\n",
              "      <td>06:52:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>30131</td>\n",
              "      <td>Child Seat eBike</td>\n",
              "      <td>355600</td>\n",
              "      <td>2019-01-01 07:01:00</td>\n",
              "      <td>302</td>\n",
              "      <td>De Angeli - Ripamonti</td>\n",
              "      <td>9</td>\n",
              "      <td>01/01/19 07:10</td>\n",
              "      <td>23</td>\n",
              "      <td>Regina Margherita</td>\n",
              "      <td>14</td>\n",
              "      <td>0:09:18</td>\n",
              "      <td>1813.18</td>\n",
              "      <td>0.3677</td>\n",
              "      <td>34.93</td>\n",
              "      <td>2019-01-01</td>\n",
              "      <td>07:01:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>21911</td>\n",
              "      <td>Bike</td>\n",
              "      <td>236069</td>\n",
              "      <td>2019-01-01 07:05:00</td>\n",
              "      <td>222</td>\n",
              "      <td>Durante - D'aviano</td>\n",
              "      <td>22</td>\n",
              "      <td>01/01/19 07:30</td>\n",
              "      <td>110</td>\n",
              "      <td>S. F. Romana</td>\n",
              "      <td>30</td>\n",
              "      <td>0:24:35</td>\n",
              "      <td>2025.83</td>\n",
              "      <td>0.4108</td>\n",
              "      <td>39.02</td>\n",
              "      <td>2019-01-01</td>\n",
              "      <td>07:05:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>10863</td>\n",
              "      <td>eBike</td>\n",
              "      <td>348357</td>\n",
              "      <td>2019-01-01 07:06:00</td>\n",
              "      <td>257</td>\n",
              "      <td>Valtellina - Aprica</td>\n",
              "      <td>13</td>\n",
              "      <td>01/01/19 08:12</td>\n",
              "      <td>262</td>\n",
              "      <td>Livigno - Monte San Genesio</td>\n",
              "      <td>15</td>\n",
              "      <td>1:06:22</td>\n",
              "      <td>887.54</td>\n",
              "      <td>0.1800</td>\n",
              "      <td>17.10</td>\n",
              "      <td>2019-01-01</td>\n",
              "      <td>07:06:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2781</td>\n",
              "      <td>Bike</td>\n",
              "      <td>147224</td>\n",
              "      <td>2019-01-01 07:07:00</td>\n",
              "      <td>154</td>\n",
              "      <td>Ascanio Sforza - Pavia</td>\n",
              "      <td>6</td>\n",
              "      <td>01/01/19 08:10</td>\n",
              "      <td>64</td>\n",
              "      <td>Diaz</td>\n",
              "      <td>36</td>\n",
              "      <td>1:02:46</td>\n",
              "      <td>2104.10</td>\n",
              "      <td>0.4267</td>\n",
              "      <td>40.53</td>\n",
              "      <td>2019-01-01</td>\n",
              "      <td>07:07:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>20347</td>\n",
              "      <td>Bike</td>\n",
              "      <td>321606</td>\n",
              "      <td>2019-01-01 07:20:00</td>\n",
              "      <td>383</td>\n",
              "      <td>Angilberto - Comacchio</td>\n",
              "      <td>29</td>\n",
              "      <td>01/01/19 07:26</td>\n",
              "      <td>303</td>\n",
              "      <td>Benaco - Brembo</td>\n",
              "      <td>30</td>\n",
              "      <td>0:06:43</td>\n",
              "      <td>1018.38</td>\n",
              "      <td>0.2065</td>\n",
              "      <td>19.62</td>\n",
              "      <td>2019-01-01</td>\n",
              "      <td>07:20:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>2253</td>\n",
              "      <td>Bike</td>\n",
              "      <td>268945</td>\n",
              "      <td>2019-01-01 07:22:00</td>\n",
              "      <td>115</td>\n",
              "      <td>Caiazzo</td>\n",
              "      <td>25</td>\n",
              "      <td>01/01/19 07:29</td>\n",
              "      <td>213</td>\n",
              "      <td>Novelli - Carnaghi</td>\n",
              "      <td>22</td>\n",
              "      <td>0:07:04</td>\n",
              "      <td>2015.42</td>\n",
              "      <td>0.4087</td>\n",
              "      <td>38.82</td>\n",
              "      <td>2019-01-01</td>\n",
              "      <td>07:22:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>20972</td>\n",
              "      <td>Bike</td>\n",
              "      <td>258180</td>\n",
              "      <td>2019-01-01 07:26:00</td>\n",
              "      <td>3</td>\n",
              "      <td>Cadorna 1</td>\n",
              "      <td>5</td>\n",
              "      <td>01/01/19 07:36</td>\n",
              "      <td>69</td>\n",
              "      <td>S. Nazaro in Brolo</td>\n",
              "      <td>13</td>\n",
              "      <td>0:09:41</td>\n",
              "      <td>2161.06</td>\n",
              "      <td>0.4383</td>\n",
              "      <td>41.63</td>\n",
              "      <td>2019-01-01</td>\n",
              "      <td>07:26:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>11345</td>\n",
              "      <td>eBike</td>\n",
              "      <td>357772</td>\n",
              "      <td>2019-01-01 07:32:00</td>\n",
              "      <td>124</td>\n",
              "      <td>Porta Venezia 2</td>\n",
              "      <td>25</td>\n",
              "      <td>01/01/19 07:42</td>\n",
              "      <td>14</td>\n",
              "      <td>San Barnaba H Mangiagalli</td>\n",
              "      <td>1</td>\n",
              "      <td>0:09:48</td>\n",
              "      <td>1859.56</td>\n",
              "      <td>0.3771</td>\n",
              "      <td>35.82</td>\n",
              "      <td>2019-01-01</td>\n",
              "      <td>07:32:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>8179</td>\n",
              "      <td>Bike</td>\n",
              "      <td>35909</td>\n",
              "      <td>2019-01-01 07:42:00</td>\n",
              "      <td>180</td>\n",
              "      <td>Canova - Sangiorgio</td>\n",
              "      <td>27</td>\n",
              "      <td>01/01/19 07:54</td>\n",
              "      <td>69</td>\n",
              "      <td>S. Nazaro in Brolo</td>\n",
              "      <td>20</td>\n",
              "      <td>0:12:29</td>\n",
              "      <td>3290.21</td>\n",
              "      <td>0.6673</td>\n",
              "      <td>63.38</td>\n",
              "      <td>2019-01-01</td>\n",
              "      <td>07:42:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>21967</td>\n",
              "      <td>Bike</td>\n",
              "      <td>354609</td>\n",
              "      <td>2019-01-01 07:43:00</td>\n",
              "      <td>196</td>\n",
              "      <td>Lagosta</td>\n",
              "      <td>29</td>\n",
              "      <td>01/01/19 07:59</td>\n",
              "      <td>315</td>\n",
              "      <td>Ospedale Maggiore</td>\n",
              "      <td>2</td>\n",
              "      <td>0:15:42</td>\n",
              "      <td>2144.11</td>\n",
              "      <td>0.4348</td>\n",
              "      <td>41.30</td>\n",
              "      <td>2019-01-01</td>\n",
              "      <td>07:43:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>1152</td>\n",
              "      <td>Bike</td>\n",
              "      <td>54371</td>\n",
              "      <td>2019-01-01 07:52:00</td>\n",
              "      <td>114</td>\n",
              "      <td>Buenos Aires - Argentina</td>\n",
              "      <td>26</td>\n",
              "      <td>01/01/19 07:57</td>\n",
              "      <td>118</td>\n",
              "      <td>VIII novembre - Ramazzini</td>\n",
              "      <td>32</td>\n",
              "      <td>0:05:30</td>\n",
              "      <td>1057.36</td>\n",
              "      <td>0.2144</td>\n",
              "      <td>20.37</td>\n",
              "      <td>2019-01-01</td>\n",
              "      <td>07:52:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>2742</td>\n",
              "      <td>Bike</td>\n",
              "      <td>355585</td>\n",
              "      <td>2019-01-01 07:59:00</td>\n",
              "      <td>14</td>\n",
              "      <td>San Barnaba H Mangiagalli</td>\n",
              "      <td>19</td>\n",
              "      <td>01/01/19 08:16</td>\n",
              "      <td>39</td>\n",
              "      <td>XXV Aprile</td>\n",
              "      <td>21</td>\n",
              "      <td>0:16:21</td>\n",
              "      <td>2619.11</td>\n",
              "      <td>0.5312</td>\n",
              "      <td>50.45</td>\n",
              "      <td>2019-01-01</td>\n",
              "      <td>07:59:00</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    BIKE_ID         BIKE_TIPE  ...  CHECK_IN_DATE_ONLY CHECK_IN_TIME_ONLY\n",
              "0     20283              Bike  ...          2019-01-01           06:52:00\n",
              "1     30131  Child Seat eBike  ...          2019-01-01           07:01:00\n",
              "2     21911              Bike  ...          2019-01-01           07:05:00\n",
              "3     10863             eBike  ...          2019-01-01           07:06:00\n",
              "4      2781              Bike  ...          2019-01-01           07:07:00\n",
              "5     20347              Bike  ...          2019-01-01           07:20:00\n",
              "6      2253              Bike  ...          2019-01-01           07:22:00\n",
              "7     20972              Bike  ...          2019-01-01           07:26:00\n",
              "8     11345             eBike  ...          2019-01-01           07:32:00\n",
              "9      8179              Bike  ...          2019-01-01           07:42:00\n",
              "10    21967              Bike  ...          2019-01-01           07:43:00\n",
              "11     1152              Bike  ...          2019-01-01           07:52:00\n",
              "12     2742              Bike  ...          2019-01-01           07:59:00\n",
              "\n",
              "[13 rows x 17 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 114
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3rJYLbsHAnMC"
      },
      "source": [
        "# Example 6: bag of words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8REd_jZnAnMC"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer \n"
      ],
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WiUC_63-AnMC",
        "outputId": "a603f526-a9e7-4d60-c138-09e1ff8f0e1b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "corpus = corpus = [\n",
        "    'Donald Trump is expected to issue more than 100 presidential pardons.',\n",
        "    'Trump is expected to end his time in office.',\n",
        "    'US defense officials say they are worried about an insider attack.',\n",
        "    'He would like to take the extraordinary step of issuing a pardon for himself']\n",
        "vectorizer = CountVectorizer()\n",
        "X = vectorizer.fit_transform(corpus)\n",
        "print(vectorizer.get_feature_names())"
      ],
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['100', 'about', 'an', 'are', 'attack', 'defense', 'donald', 'end', 'expected', 'extraordinary', 'for', 'he', 'himself', 'his', 'in', 'insider', 'is', 'issue', 'issuing', 'like', 'more', 'of', 'office', 'officials', 'pardon', 'pardons', 'presidential', 'say', 'step', 'take', 'than', 'the', 'they', 'time', 'to', 'trump', 'us', 'worried', 'would']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UML0FScHAnMC",
        "outputId": "015bdb2a-f33f-48ad-db68-21ffb45816af",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(X.toarray())"
      ],
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 1 1 0 0 1 0 0 0 0 1 1 0 0 0 1 0 0 0 1 1\n",
            "  0 0 0]\n",
            " [0 0 0 0 0 0 0 1 1 0 0 0 0 1 1 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 1 1\n",
            "  0 0 0]\n",
            " [0 1 1 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 1 0 0 0\n",
            "  1 1 0]\n",
            " [0 0 0 0 0 0 0 0 0 1 1 1 1 0 0 0 0 0 1 1 0 1 0 0 1 0 0 0 1 1 0 1 0 0 1 0\n",
            "  0 0 1]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FwLThb0JAnMD",
        "outputId": "8df31e29-394a-466f-da58-cfd04602ab08",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# This time we use 2-grams:\n",
        "vectorizer2 = CountVectorizer(analyzer='word', ngram_range=(2, 2))\n",
        "X2 = vectorizer2.fit_transform(corpus)\n",
        "print(vectorizer2.get_feature_names())\n"
      ],
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['100 presidential', 'about an', 'an insider', 'are worried', 'defense officials', 'donald trump', 'end his', 'expected to', 'extraordinary step', 'for himself', 'he would', 'his time', 'in office', 'insider attack', 'is expected', 'issue more', 'issuing pardon', 'like to', 'more than', 'of issuing', 'officials say', 'pardon for', 'presidential pardons', 'say they', 'step of', 'take the', 'than 100', 'the extraordinary', 'they are', 'time in', 'to end', 'to issue', 'to take', 'trump is', 'us defense', 'worried about', 'would like']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dqnDSOJ_AnME",
        "outputId": "2dc3fd21-08fc-47b9-a047-190832824ef9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(X2.toarray())"
      ],
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1 0 0 0 0 1 0 1 0 0 0 0 0 0 1 1 0 0 1 0 0 0 1 0 0 0 1 0 0 0 0 1 0 1 0 0\n",
            "  0]\n",
            " [0 0 0 0 0 0 1 1 0 0 0 1 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 1 0 0\n",
            "  0]\n",
            " [0 1 1 1 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 1 0 0 0 0 1 0 0 0 0 0 1 1\n",
            "  0]\n",
            " [0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 1 1 0 1 0 1 0 0 1 1 0 1 0 0 0 0 1 0 0 0\n",
            "  1]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "smvjEK0yAnME"
      },
      "source": [
        "# Example 7: TF-IDF"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mJQ4uuxnAnMF",
        "outputId": "266015d0-cef8-4645-9c49-1485c1621a4f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "vectorizer = TfidfVectorizer()\n",
        "X = vectorizer.fit_transform(corpus)\n",
        "print(vectorizer.get_feature_names())"
      ],
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['100', 'about', 'an', 'are', 'attack', 'defense', 'donald', 'end', 'expected', 'extraordinary', 'for', 'he', 'himself', 'his', 'in', 'insider', 'is', 'issue', 'issuing', 'like', 'more', 'of', 'office', 'officials', 'pardon', 'pardons', 'presidential', 'say', 'step', 'take', 'than', 'the', 'they', 'time', 'to', 'trump', 'us', 'worried', 'would']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C8GriAdOAnMF",
        "outputId": "fc186a63-75b5-4039-9430-125abadbe70e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(X.shape)"
      ],
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(4, 39)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "tXR8c7yvAnMF",
        "outputId": "80d7d607-4845-4169-de88-8ebc4a6a6710",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(X.toarray())# Example 8: Practice with SpaCy.toarray())"
      ],
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.32840433 0.         0.         0.         0.         0.\n",
            "  0.32840433 0.         0.25891775 0.         0.         0.\n",
            "  0.         0.         0.         0.         0.25891775 0.32840433\n",
            "  0.         0.         0.32840433 0.         0.         0.\n",
            "  0.         0.32840433 0.32840433 0.         0.         0.\n",
            "  0.32840433 0.         0.         0.         0.20961623 0.25891775\n",
            "  0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.37082366 0.29236164 0.         0.         0.\n",
            "  0.         0.37082366 0.37082366 0.         0.29236164 0.\n",
            "  0.         0.         0.         0.         0.37082366 0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.37082366 0.23669194 0.29236164\n",
            "  0.         0.         0.        ]\n",
            " [0.         0.30151134 0.30151134 0.30151134 0.30151134 0.30151134\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.30151134 0.         0.\n",
            "  0.         0.         0.         0.         0.         0.30151134\n",
            "  0.         0.         0.         0.30151134 0.         0.\n",
            "  0.         0.         0.30151134 0.         0.         0.\n",
            "  0.30151134 0.30151134 0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.2838961  0.2838961  0.2838961\n",
            "  0.2838961  0.         0.         0.         0.         0.\n",
            "  0.2838961  0.2838961  0.         0.2838961  0.         0.\n",
            "  0.2838961  0.         0.         0.         0.2838961  0.2838961\n",
            "  0.         0.2838961  0.         0.         0.1812072  0.\n",
            "  0.         0.         0.2838961 ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tvi9ijJFAnMF"
      },
      "source": [
        "# Example 8: Practice with SpaCy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oCiq-S8ZAnMF"
      },
      "source": [
        "# SpaCy\n",
        "\n",
        "spaCy is an open-source software library for advanced natural language processing: https://spacy.io/\n",
        "\n",
        "The following code is based on: https://medium.com/@ageitgey/natural-language-processing-is-fun-9a0bff37854e"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Oau-6geAnMF"
      },
      "source": [
        "import spacy\n",
        "import textacy.extract\n",
        "from urllib import request\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# Load the large English NLP model\n",
        "nlp = spacy.load('en_core_web_sm')"
      ],
      "execution_count": 123,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E86B9yNUAnMG"
      },
      "source": [
        "## Extracting Facts from text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s5eCchtEAnMH"
      },
      "source": [
        "def print_facts(keyword, url):\n",
        "\n",
        "    # fetch url\n",
        "    response = request.urlopen(url)\n",
        "    \n",
        "    # read html in utf8\n",
        "    html = response.read().decode('utf8')\n",
        "    \n",
        "    # strip html and get raw text\n",
        "    raw = BeautifulSoup(html, 'html.parser').get_text()\n",
        "    \n",
        "    # you should do some pre-processing...\n",
        "    text = raw.replace('\\n',' ')\n",
        "    \n",
        "    # Parse the document with spaCy\n",
        "    doc = nlp(text)\n",
        "\n",
        "    # Extract semi-structured statements\n",
        "    statements = textacy.extract.semistructured_statements(doc, keyword)\n",
        "\n",
        "    # Print the results\n",
        "    if keyword == 'Biden':\n",
        "        print(\"Here are the things I know about Biden:\\n\")\n",
        "        for statement in statements:\n",
        "            subject, verb, fact = statement\n",
        "            print(f\" - {fact}\")\n",
        "    else:\n",
        "        print(\"Here are the things I know about Trump:\\n\")\n",
        "        for statement in statements:\n",
        "            subject, verb, fact = statement\n",
        "            print(f\" - {fact}\")\n",
        "    return"
      ],
      "execution_count": 124,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y326h5m7AnMH",
        "outputId": "eab918c6-3bfa-4d5e-a702-38440c2dfd8a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# print facts about London fetching the wikipedia page\n",
        "print_facts(\"Biden\", \"https://en.wikipedia.org/wiki/Joe_Biden\")"
      ],
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Here are the things I know about Biden:\n",
            "\n",
            " - a longtime member of the Senate Foreign Relations Committee and eventually became its chairman\n",
            " - the oldest elected president, the first from Delaware, and the second Catholic\n",
            " - a Washington lobbyist and investment adviser.[69\n",
            " - one of the Senate's strongest opponents of race-integration busing\n",
            " - a longtime member of the Senate Foreign Relations Committee\n",
            " - not an academic\n",
            " - the kind of fundamentally happy person who can be as generous toward others as he is to himself\n",
            " - his running mate.[178\n",
            " - satisfied that no major instances of waste or corruption had occurred,[176] and\n",
            " - always prepared to be the skunk at the family picnic to make sure we are as intellectually honest as possible\n",
            " - the only guy with real negotiating authority, and\n",
            " - still uncertain about running\n",
            " - a staunch supporter of the Affordable Care Act (ACA).[421][422\n",
            " - an inductee of the Delaware Volunteer Firemen's Association Hall of Fame.[469\n",
            " - eligible to receive classified intelligence briefings since his nomination in August.[391\n",
            " - the stuttering kid who wanted the ball\n",
            " - a regular Joe\n",
            " - a Lifeline for a Rail Agency in Crisis\n",
            " - There, Done\n",
            " - vice president\n",
            " - the longest-serving vice president to never do so.  \n",
            " - The Front-Runner, But There's No Clear Favorite\n",
            " - 'creepy\n",
            " - Off To A Record Start\n",
            " - now a Bernie Bro\n",
            " - Wrong On the Cold War\n",
            " - MSU's Spring 2017 Commencement Speaker\n",
            " - TIME's 2020 Person of the Year\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jFXEFEszAnMH",
        "outputId": "4d0fd248-8421-43ab-a12a-3db969ae437d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# print facts about Trump fetching the wikipedia page\n",
        "print_facts(\"Trump\", \"https://en.wikipedia.org/wiki/Donald_Trump\")"
      ],
      "execution_count": 126,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Here are the things I know about Trump:\n",
            "\n",
            " - a millionaire by age 8\n",
            " - the healthiest individual ever elected to the presidency\" in a letter released by the Trump campaign.[62\n",
            " - a guest about 24 times on the nationally syndicated Howard Stern Show.[169\n",
            " - the oldest person to take office as president at the time of his inauguration\n",
            " - slow to appoint second-tier officials in the executive branch, saying many of the positions are unnecessary\n",
            " - the subject of increasing Justice Department and congressional scrutiny, with investigations covering his election campaign, transition and inauguration\n",
            " - still in office\n",
            " - \"practically and morally responsible for provoking the events of the day\" but \"constitutionally not eligible for conviction\n",
            " - the first elected president not to be named most admired in his first year in office.[758\n",
            " - the subject of parody, comedy, and caricature\n",
            " - not an 'active member\n",
            " - a teetotaler\n",
            " - Sicker Than Acknowledged With Covid-19\n",
            " - so rich\n",
            " - a Democrat longer than a Republican 'in the last decade\n",
            " - our nominee\n",
            " - the first U.S. president with no government or military experience\n",
            " - the only US president ever with no political or military experience\n",
            " - right that with lower testing, we record fewer cases\n",
            " - Officially under Investigation\n",
            " - right that 'collusion is not a crime\n",
            " - in No Mood To Concede, But Says Will Leave White House\n",
            " - \"practically and morally responsible\" for riot after voting not guilty\n",
            " - nation's single largest spreader of disinformation, studies say\n",
            " - a racist\n",
            " - Trump (2017\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-U-eNmVNAnMH"
      },
      "source": [
        "## What else can we do?\n",
        "\n",
        "Imagine that you were building a website that let’s the user view information for every city in the world using the information we extracted in the last example. If you had a search feature on the website, it might be nice to __autocomplete__ common search queries like Google does. But to do this, we need a list of possible completions to suggest to the user. We can use NLP to quickly generate this data. Here’s one way to extract frequently-mentioned noun chunks from a document."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h-oTjn8HAnMR"
      },
      "source": [
        "def autocomplete(keyword, url, min_freq):\n",
        "    \n",
        "    # fetch url\n",
        "    response = request.urlopen(url)\n",
        "    \n",
        "    # read html in utf8\n",
        "    html = response.read().decode('utf8')\n",
        "    \n",
        "    # strip html and get raw text\n",
        "    raw = BeautifulSoup(html, 'html.parser').get_text()\n",
        "    \n",
        "    # you should do some pre-processing...\n",
        "    text = raw.replace('\\n',' ')\n",
        "    \n",
        "    # Parse the document with spaCy\n",
        "    doc = nlp(text)\n",
        "\n",
        "    # Extract noun chunks that appear\n",
        "    noun_chunks = textacy.extract.noun_chunks(doc, min_freq = min_freq)\n",
        "\n",
        "    # Convert noun chunks to lowercase strings\n",
        "    noun_chunks = map(str, noun_chunks)\n",
        "    noun_chunks = map(str.lower, noun_chunks)\n",
        "\n",
        "    # Collect any nouns that are at least 2 words long\n",
        "    res = []\n",
        "    for noun_chunk in set(noun_chunks):\n",
        "        if len(noun_chunk.split(\" \")) > 1:\n",
        "            res.append(noun_chunk)\n",
        "        \n",
        "    return res"
      ],
      "execution_count": 127,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vcuO06h2AnMR",
        "outputId": "b3cb9487-7b78-4faf-ecba-4d5b9ad04650",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# autocomplete Biden\n",
        "autocomplete(\"Biden\", \"https://en.wikipedia.org/wiki/Joe_Biden\", 7)"
      ],
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['united states',\n",
              " 'usa today',\n",
              " 'abc news',\n",
              " 'joe biden',\n",
              " 'washington post',\n",
              " 'vice president',\n",
              " 'barack obama',\n",
              " 'senate foreign relations committee',\n",
              " 'associated press',\n",
              " 'wall street journal',\n",
              " 'los angeles times',\n",
              " 'donald trump',\n",
              " 'wayback machine',\n",
              " 'new york times',\n",
              " 'fox news',\n",
              " 'running mate',\n",
              " 'foreign relations',\n",
              " 'retrieved february',\n",
              " '^ biden',\n",
              " 'news journal',\n",
              " 'nbc news',\n",
              " 'foreign policy',\n",
              " 'white house',\n",
              " 'u.s. senate',\n",
              " 'second term']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 128
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "15_zCKvzAnMS",
        "outputId": "500dbd7a-9183-423d-e889-d86ab9ed910c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# autocomplete Trump\n",
        "autocomplete(\"Trump\", \"https://en.wikipedia.org/wiki/Donald_Trump\", 10)"
      ],
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['united states',\n",
              " 'abc news',\n",
              " 'usa today',\n",
              " 'washington post',\n",
              " 'mueller report',\n",
              " 'trump administration',\n",
              " 'north korea',\n",
              " 'supreme court',\n",
              " 'new york',\n",
              " 'russian government',\n",
              " 'cbs news',\n",
              " 'ap news',\n",
              " 'associated press',\n",
              " 'wall street journal',\n",
              " 'donald trump',\n",
              " 'trump organization',\n",
              " 'new york times',\n",
              " 'president trump',\n",
              " 'main article',\n",
              " 'michael d.',\n",
              " 'russian interference',\n",
              " 'retrieved february',\n",
              " 'nbc news',\n",
              " '^ kranish',\n",
              " 'trump campaign',\n",
              " 'white house',\n",
              " 'potentially dated statements',\n",
              " 'bbc news']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 129
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1TRp4CSiAnMS"
      },
      "source": [
        "## Example 9: PCA in text mining"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vYFv1r7RAnMS",
        "outputId": "ea935c73-1def-4b65-dd70-640af83afe78",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "def vectorizing(data):\n",
        "    vec = CountVectorizer()\n",
        "    X = vec.fit_transform(data)\n",
        "    df = pd.DataFrame(X.toarray(), columns=vec.get_feature_names())\n",
        "    return df\n",
        "\n",
        "def find_principal_components(n, data):\n",
        "    pca = PCA(n_components = n)\n",
        "    principalComponents = pca.fit_transform(data)\n",
        "    return pd.DataFrame(pca.components_, columns=data.columns)\n",
        "\n",
        "text = ['Texas real estate agent Ryan Williams',\n",
        "        'part mob Trump storm administration Capitol congress continue insist innocence',\n",
        "        'even face charge breach Capitol guilt heart Ryan', \n",
        "        'tell today Pelosi show glad Ryan Williams there because witness history Trump administration', \n",
        "        'never get  chance do again Texas Capitol there mob', \n",
        "        'storm Pelosi laptop invade office congress Pelosi Trump Biden prison guilt prison breach steal laptop',\n",
        "        'Trump Williams Biden Trump president elect president Trump']\n",
        "\n",
        "df = vectorizing(text)\n",
        "\n",
        "print(df) # 7 row x 44 columns"
      ],
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "   administration  again  agent  because  ...  today  trump  williams  witness\n",
            "0               0      0      1        0  ...      0      0         1        0\n",
            "1               1      0      0        0  ...      0      1         0        0\n",
            "2               0      0      0        0  ...      0      0         0        0\n",
            "3               1      0      0        1  ...      1      1         1        1\n",
            "4               0      1      0        0  ...      0      0         0        0\n",
            "5               0      0      0        0  ...      0      1         0        0\n",
            "6               0      0      0        0  ...      0      3         1        0\n",
            "\n",
            "[7 rows x 44 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BJ-QNncRAnMT",
        "outputId": "3ff25e25-95b1-4c8b-8ead-ec2fdf4cc330",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "principalDF = find_principal_components(2, df)\n",
        "\n",
        "print(principalDF) # 2 rows x 44 columns\n"
      ],
      "execution_count": 131,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "   administration     again     agent  ...     trump  williams   witness\n",
            "0       -0.047409 -0.083647 -0.061637  ...  0.221641 -0.070288 -0.029229\n",
            "1        0.051772 -0.084410 -0.022809  ...  0.629815  0.255983  0.062849\n",
            "\n",
            "[2 rows x 44 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dP2P45asAnMT"
      },
      "source": [
        ""
      ],
      "execution_count": 131,
      "outputs": []
    }
  ]
}